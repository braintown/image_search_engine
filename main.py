import os
import json
import torch
import numpy as np
import pandas as pd
from PIL import Image
import cv2
import logging
from pathlib import Path
from typing import List, Dict, Union, Tuple, Optional
import chromadb
from chromadb.config import Settings
import clip
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import hashlib
import base64
from io import BytesIO
import requests
import pymysql
from urllib.parse import urlparse
import warnings
import time
import re

# å¿½ç•¥ä¸€äº›è­¦å‘Š
warnings.filterwarnings("ignore", category=UserWarning)

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# é…ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

# è®¾å¤‡é…ç½®
device = "cuda" if torch.cuda.is_available() else "cpu"
logger.info(f"ä½¿ç”¨è®¾å¤‡: {device}")

class OpenRouterProcessor:
    """OpenRouterå¤§è¯­è¨€æ¨¡å‹å¤„ç†å™¨"""
    
    def __init__(self, api_key: str, model: str = "anthropic/claude-3-haiku"):
        """
        åˆå§‹åŒ–OpenRouterå¤„ç†å™¨
        Args:
            api_key: OpenRouter APIå¯†é’¥
            model: ä½¿ç”¨çš„æ¨¡å‹åç§°
        """
        self.api_key = api_key
        self.model = model
        self.base_url = "https://openrouter.ai/api/v1/chat/completions"
        self.headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json",
            "HTTP-Referer": "https://github.com/your-repo",  # å¯é€‰
            "X-Title": "CLIP Image Search System"  # å¯é€‰
        }
        
        # æµ‹è¯•è¿æ¥
        self._test_connection()
    
    def _test_connection(self):
        """æµ‹è¯•APIè¿æ¥"""
        try:
            test_response = self.analyze_query(
                "æµ‹è¯•è¿æ¥", 
                ["æµ‹è¯•"], 
                max_tokens=10,
                timeout=5
            )
            logger.info("âœ… OpenRouterè¿æ¥æµ‹è¯•æˆåŠŸ")
        except Exception as e:
            logger.warning(f"âš ï¸ OpenRouterè¿æ¥æµ‹è¯•å¤±è´¥: {e}")
    
    def analyze_query(self, user_query: str, available_tags: List[str], 
                     max_tokens: int = 500, timeout: int = 10) -> Dict:
        """
        åˆ†æç”¨æˆ·æŸ¥è¯¢å¹¶æå–ç›¸å…³æ ‡ç­¾å’Œå…³é”®è¯
        Args:
            user_query: ç”¨æˆ·çš„è‡ªç„¶è¯­è¨€æŸ¥è¯¢
            available_tags: å¯ç”¨çš„æ ‡ç­¾åˆ—è¡¨
            max_tokens: æœ€å¤§tokenæ•°
            timeout: è¶…æ—¶æ—¶é—´
        """
        try:
            # æ„å»ºæç¤ºè¯
            prompt = self._build_analysis_prompt(user_query, available_tags)
            
            payload = {
                "model": self.model,
                "messages": [
                    {
                        "role": "system",
                        "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ±½è½¦å›¾ç‰‡æœç´¢åŠ©æ‰‹ï¼Œæ“…é•¿ç†è§£ç”¨æˆ·çš„è‡ªç„¶è¯­è¨€æè¿°å¹¶æå–ç›¸å…³çš„è§†è§‰ç‰¹å¾å’Œæ ‡ç­¾ã€‚"
                    },
                    {
                        "role": "user", 
                        "content": prompt
                    }
                ],
                "max_tokens": max_tokens,
                "temperature": 0.3,
                "top_p": 0.9
            }
            
            response = requests.post(
                self.base_url,
                headers=self.headers,
                json=payload,
                timeout=timeout
            )
            
            if response.status_code == 200:
                result = response.json()
                content = result['choices'][0]['message']['content']
                
                # è§£æLLMçš„å›å¤
                analysis = self._parse_llm_response(content)
                
                logger.info(f"OpenRouteråˆ†æå®Œæˆ: {analysis.get('summary', '')}")
                return analysis
                
            else:
                logger.error(f"OpenRouter APIé”™è¯¯: {response.status_code} - {response.text}")
                return self._fallback_analysis(user_query, available_tags)
                
        except Exception as e:
            logger.error(f"OpenRouteråˆ†æå¤±è´¥: {e}")
            return self._fallback_analysis(user_query, available_tags)
    
    def _build_analysis_prompt(self, user_query: str, available_tags: List[str]) -> str:
        """æ„å»ºåˆ†ææç¤ºè¯"""
        
        # å°†æ ‡ç­¾æŒ‰ç±»åˆ«ç»„ç»‡ï¼ˆåŸºäºæ‚¨æä¾›çš„tag_keywordsç»“æ„ï¼‰
        tag_categories = {
            "è‰²å½©": ["å•è‰²ç³»", "å¯¹æ¯”è‰²", "é»‘ç™½", "é‡‘å±è‰²", "å“‘å…‰è‰²", "é²œè‰³è‰²å½©", "æŸ”å’Œè‰²å½©", "å¤å¤è‰²å½©", "æ¢¦å¹»è‰²å½©"],
            "è‰²è°ƒ":["å†·è‰²è°ƒ", "æš–è‰²è°ƒ", "ä¸­æ€§è‰²è°ƒ", "é«˜å¯¹æ¯”åº¦", "ä½å¯¹æ¯”åº¦", "æ˜äº®è‰²è°ƒ", "æš—é»‘è‰²è°ƒ", "é»„æ˜è‰²è°ƒ", "è¤ªè‰²æ•ˆæœ", "å¤œæ™¯è‰²è°ƒ", "é¥±å’Œè‰²è°ƒ"],
            "å…‰çº¿":["è‡ªç„¶å…‰çº¿", "äººå·¥å…‰çº¿", "æŸ”å’Œå…‰çº¿", "å¼ºçƒˆå…‰çº¿", "ä¾§å…‰", "é€†å…‰", "é¡ºå…‰", "æ¼«å°„å…‰", "å…‰å½±å¯¹æ¯”", "é»„é‡‘æ—¶åˆ»å…‰çº¿", "è“è°ƒæ—¶åˆ»å…‰çº¿", "å¤œæ™šå…‰çº¿"],
            "æ„å›¾":["ä¸­å¿ƒæ„å›¾", "å¯¹ç§°æ„å›¾", "ä¸‰åˆ†æ³•æ„å›¾", "å‰æ™¯æ¡†æ¶", "å¼•å¯¼çº¿æ„å›¾", "é‡å¤å…ƒç´ ", "è´Ÿç©ºé—´æ„å›¾", "å¯¹è§’çº¿æ„å›¾", "å±‚æ¬¡æ„å›¾", "æœ€å°åŒ–æ„å›¾", "é»„é‡‘æ¯”ä¾‹æ„å›¾"],
            "è´¨æ„Ÿ":["é‡‘å±è´¨æ„Ÿ", "å…‰æ»‘è´¨æ„Ÿ", "å“‘å…‰è´¨æ„Ÿ", "ç²—ç³™è´¨æ„Ÿ", "åå…‰è´¨æ„Ÿ", "çš®é©è´¨æ„Ÿ", "ç§‘æŠ€è´¨æ„Ÿ", "å¥¢åè´¨æ„Ÿ", "å¤å¤è´¨æ„Ÿ", "è‡ªç„¶è´¨æ„Ÿ"],
            "äººè½¦äº’åŠ¨":["ç”Ÿæ´»", "å®¶åº­", "ä¼‘é—²", "è¡—æ‹", "åŸå¸‚", "é£æ™¯", "å»ºç­‘", "é©¾é©¶åœºæ™¯", "å®¶åº­å‡ºæ¸¸", "å•†åŠ¡å‡ºè¡Œ", "ä¼‘é—²æ—…è¡Œ", "æˆ·å¤–æ¢é™©", "åŸå¸‚é€šå‹¤", "ç¤¾äº¤èšä¼š", "å±•ç¤ºåœºæ™¯", "è¯•é©¾åœºæ™¯", "å„¿ç«¥äº’åŠ¨", "å® ç‰©äº’åŠ¨", "æƒ…ä¾£åœºæ™¯"],
            "ç”»é¢é£æ ¼":["æ‘„å½±", "CG", "æç®€", "å•†ä¸šé£æ ¼", "ç”Ÿæ´»çºªå®", "å¤å¤é£æ ¼", "æœªæ¥é£æ ¼", "è‰ºæœ¯åˆ›æ„", "å·¥ä¸šé£æ ¼", "è¿åŠ¨é£æ ¼", "å¥¢åé£æ ¼", "ç§‘æŠ€é£æ ¼", "ç”µå½±æ„Ÿ"],
            "æ‹æ‘„è§†è§’":["ç‰¹å†™","æ­£é¢è§†è§’", "ä¾§é¢è§†è§’", "45åº¦è§’", "åè§†å›¾", "ä¿¯è§†å›¾", "ä»°è§†è§’", "è½¦å†…è§†è§’", "å…¨æ™¯è§†è§’", "é¸Ÿç°è§†è§’",],
            "è½¦å‹": ["è½¿è½¦", "SUV", "è¶Šé‡", "æˆ¿è½¦", "MPV", "ç´§å‡‘å‹è½¿è½¦", "ä¸­å‹è½¿è½¦", "è±ªåè½¿è½¦", "è·‘è½¦", "çš®å¡", "å¤å…¸è½¦", "ç”µåŠ¨è½¦"],
        }
        
        # ä»å¯ç”¨æ ‡ç­¾ä¸­ç­›é€‰æ¯ä¸ªç±»åˆ«çš„æ ‡ç­¾
        available_by_category = {}
        for category, category_tags in tag_categories.items():
            available_by_category[category] = [tag for tag in category_tags if tag in available_tags]
        
        prompt = f"""
ç”¨æˆ·æŸ¥è¯¢ï¼š"{user_query}"

è¯·åˆ†æè¿™ä¸ªæŸ¥è¯¢ï¼Œå¹¶ä»¥JSONæ ¼å¼è¿”å›ä»¥ä¸‹ä¿¡æ¯ï¼š

1. **summary**: ç”¨1-2å¥è¯æ€»ç»“ç”¨æˆ·æƒ³è¦æ‰¾ä»€ä¹ˆæ ·çš„æ±½è½¦å›¾ç‰‡
2. **key_concepts**: æå–3-5ä¸ªæ ¸å¿ƒæ¦‚å¿µ
3. **visual_keywords**: é€‚åˆCLIPè§†è§‰æœç´¢çš„è‹±æ–‡å…³é”®è¯ï¼ˆ3-5ä¸ªï¼‰
4. **matched_tags**: ä»ä¸‹é¢çš„æ ‡ç­¾åº“ä¸­é€‰æ‹©æœ€ç›¸å…³çš„æ ‡ç­¾ï¼ˆæ¯ä¸ªç±»åˆ«æœ€å¤š3ä¸ªï¼‰
5. **scene_type**: ä¸»è¦åœºæ™¯ç±»å‹
6. **style_preference**: é£æ ¼åå¥½
7. **search_strategy**: å»ºè®®çš„æœç´¢ç­–ç•¥ï¼ˆ"tag_focused"ã€"visual_focused"æˆ–"balanced"ï¼‰

å¯ç”¨æ ‡ç­¾åº“ï¼š
{json.dumps(available_by_category, ensure_ascii=False, indent=2)}

è¯·è¿”å›æ ‡å‡†çš„JSONæ ¼å¼ï¼Œç¡®ä¿æ‰€æœ‰å­—æ®µéƒ½å­˜åœ¨ã€‚å¦‚æœæŸä¸ªç±»åˆ«æ²¡æœ‰åŒ¹é…çš„æ ‡ç­¾ï¼Œè¿”å›ç©ºåˆ—è¡¨ã€‚

ç¤ºä¾‹è¾“å‡ºæ ¼å¼ï¼š
{{
    "summary": "ç”¨æˆ·æƒ³è¦å¯»æ‰¾å®¶åº­æ¸©é¦¨åœºæ™¯çš„æ±½è½¦å›¾ç‰‡",
    "key_concepts": ["å®¶åº­", "æ¸©é¦¨", "ä¼‘é—²"],
    "visual_keywords": ["family car", "warm lighting", "casual scene"],
    "matched_tags": {{
            "è‰²å½©": [],
            "è‰²è°ƒ":[],
            "å…‰çº¿":["è‡ªç„¶å…‰çº¿"],
            "æ„å›¾":["ä¸­å¿ƒæ„å›¾"],
            "è´¨æ„Ÿ":[],
            "äººè½¦äº’åŠ¨":["ç”Ÿæ´»"],
            "ç”»é¢é£æ ¼":["æç®€"],
            "æ‹æ‘„è§†è§’":["æ­£é¢è§†è§’"],
            "è½¦å‹": ["è½¿è½¦"],
    }},
    "scene_type": "å®¶åº­ç”Ÿæ´»",
    "style_preference": "æ¸©é¦¨è‡ªç„¶",
    "search_strategy": "balanced"
}}
"""
        return prompt
    
    def _parse_llm_response(self, content: str) -> Dict:
        """è§£æLLMè¿”å›çš„å†…å®¹"""
        try:
            # å°è¯•ç›´æ¥è§£æJSON
            if content.strip().startswith('{') and content.strip().endswith('}'):
                return json.loads(content)
            
            # å°è¯•æå–JSONéƒ¨åˆ†
            json_match = re.search(r'\{.*\}', content, re.DOTALL)
            if json_match:
                json_str = json_match.group(0)
                return json.loads(json_str)
            
            # å¦‚æœæ— æ³•è§£æJSONï¼Œè¿”å›åŸºç¡€åˆ†æ
            logger.warning("æ— æ³•è§£æLLMè¿”å›çš„JSONï¼Œä½¿ç”¨æ–‡æœ¬è§£æ")
            return self._parse_text_response(content)
            
        except Exception as e:
            logger.error(f"è§£æLLMå“åº”å¤±è´¥: {e}")
            return self._create_default_analysis(content)
    
    def _parse_text_response(self, content: str) -> Dict:
        """è§£ææ–‡æœ¬å½¢å¼çš„å“åº”"""
        analysis = {
            "summary": "",
            "key_concepts": [],
            "visual_keywords": [],
            "matched_tags": {},
            "scene_type": "",
            "style_preference": "",
            "search_strategy": "balanced"
        }
        
        # ç®€å•çš„æ–‡æœ¬è§£æé€»è¾‘
        lines = content.split('\n')
        current_section = None
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
                
            if 'summary' in line.lower() or 'æ€»ç»“' in line:
                current_section = 'summary'
            elif 'key_concepts' in line.lower() or 'æ ¸å¿ƒæ¦‚å¿µ' in line:
                current_section = 'key_concepts'
            elif 'visual_keywords' in line.lower() or 'è§†è§‰å…³é”®è¯' in line:
                current_section = 'visual_keywords'
            elif current_section and ':' in line:
                value = line.split(':', 1)[1].strip()
                if current_section == 'summary':
                    analysis['summary'] = value
                elif current_section in ['key_concepts', 'visual_keywords']:
                    # åˆ†å‰²å¹¶æ¸…ç†
                    items = [item.strip() for item in value.replace(',', 'ï¼Œ').split('ï¼Œ') if item.strip()]
                    analysis[current_section] = items
        
        return analysis
    
    def _create_default_analysis(self, content: str) -> Dict:
        """åˆ›å»ºé»˜è®¤åˆ†æç»“æœ"""
        return {
            "summary": content[:100] if content else "æ— æ³•è§£ææŸ¥è¯¢",
            "key_concepts": [],
            "visual_keywords": [],
            "matched_tags": {},
            "scene_type": "æœªçŸ¥",
            "style_preference": "æœªçŸ¥", 
            "search_strategy": "balanced"
        }
    
    def _fallback_analysis(self, user_query: str, available_tags: List[str]) -> Dict:
        """å¤‡ç”¨åˆ†ææ–¹æ³•ï¼ˆä¸ä¾èµ–LLMï¼‰"""
        logger.info("ä½¿ç”¨å¤‡ç”¨åˆ†ææ–¹æ³•")
        
        # ç®€å•çš„å…³é”®è¯åŒ¹é…
        query_lower = user_query.lower()
        matched_tags = []
        
        for tag in available_tags:
            if tag.lower() in query_lower:
                matched_tags.append(tag)
        
        # åŸºç¡€åœºæ™¯è¯†åˆ«
        scene_keywords = {
            "å®¶åº­": ["å®¶åº­", "äº²å­", "å®¶äºº", "æ¸©é¦¨"],
            "å•†åŠ¡": ["å•†åŠ¡", "åŠå…¬", "å·¥ä½œ", "æ­£å¼"],
            "ä¼‘é—²": ["ä¼‘é—²", "æ”¾æ¾", "åº¦å‡", "å¨±ä¹"],
            "åŸå¸‚": ["åŸå¸‚", "éƒ½å¸‚", "è¡—é“", "å¸‚åŒº"],
            "è‡ªç„¶": ["è‡ªç„¶", "é£æ™¯", "æˆ·å¤–", "å±±", "æµ·"]
        }
        
        detected_scene = "æœªçŸ¥"
        for scene, keywords in scene_keywords.items():
            if any(keyword in query_lower for keyword in keywords):
                detected_scene = scene
                break
        
        return {
            "summary": f"æŸ¥è¯¢: {user_query}",
            "key_concepts": user_query.split()[:5],
            "visual_keywords": user_query.split()[:3],
            "matched_tags": {"é€šç”¨": matched_tags[:5]},
            "scene_type": detected_scene,
            "style_preference": "æœªæŒ‡å®š",
            "search_strategy": "balanced"
        }

class CLIPImageEncoder:
    """CLIPå›¾åƒå’Œæ–‡æœ¬ç¼–ç å™¨ - å¢å¼ºç‰ˆ"""
    
    SUPPORTED_MODELS = [
        "ViT-B/32", "ViT-B/16", "ViT-L/14", "ViT-L/14@336px",
        "RN50", "RN101", "RN50x4", "RN50x16", "RN50x64"
    ]
    
    def __init__(self, model_name: str = "ViT-B/32"):
        """
        åˆå§‹åŒ–CLIPæ¨¡å‹
        Args:
            model_name: CLIPæ¨¡å‹åç§°
        """
        if model_name not in self.SUPPORTED_MODELS:
            logger.warning(f"æ¨¡å‹ {model_name} å¯èƒ½ä¸å—æ”¯æŒï¼Œæ”¯æŒçš„æ¨¡å‹: {self.SUPPORTED_MODELS}")
        
        self.model_name = model_name
        self.device = device
        
        try:
            self.model, self.preprocess = clip.load(model_name, device=self.device)
            self.model.eval()
            logger.info(f"CLIPæ¨¡å‹ {model_name} å·²åŠ è½½åˆ° {self.device}")
            
            # è·å–ç‰¹å¾ç»´åº¦
            with torch.no_grad():
                dummy_image = torch.randn(1, 3, 224, 224).to(self.device)
                dummy_features = self.model.encode_image(dummy_image)
                self.feature_dim = dummy_features.shape[1]
            
            logger.info(f"ç‰¹å¾ç»´åº¦: {self.feature_dim}")
            
        except Exception as e:
            logger.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise
    
    def encode_image_from_path(self, image_path: str) -> Optional[np.ndarray]:
        """
        ä»æœ¬åœ°æ–‡ä»¶è·¯å¾„ç¼–ç å›¾ç‰‡
        Args:
            image_path: å›¾ç‰‡çš„ç»å¯¹è·¯å¾„
        Returns:
            å›¾ç‰‡ç‰¹å¾å‘é‡
        """
        try:
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not os.path.exists(image_path):
                logger.warning(f"å›¾ç‰‡æ–‡ä»¶ä¸å­˜åœ¨: {image_path}")
                return None
            
            # åŠ è½½å’Œé¢„å¤„ç†å›¾ç‰‡
            image = Image.open(image_path).convert('RGB')
            image_input = self.preprocess(image).unsqueeze(0).to(self.device)
            
            # ç¼–ç å›¾ç‰‡
            with torch.no_grad():
                image_features = self.model.encode_image(image_input)
                image_features = image_features / image_features.norm(dim=-1, keepdim=True)
                
            return image_features.cpu().numpy()[0]
            
        except Exception as e:
            logger.error(f"å›¾ç‰‡ç¼–ç å¤±è´¥ {image_path}: {e}")
            return None
    
    def encode_image(self, image_path: str) -> Optional[np.ndarray]:
        """
        ç¼–ç å•å¼ å›¾ç‰‡ï¼ˆåˆ«åæ–¹æ³•ï¼Œç”¨äºä»¥å›¾æœå›¾ï¼‰
        Args:
            image_path: å›¾ç‰‡è·¯å¾„
        Returns:
            å›¾ç‰‡ç‰¹å¾å‘é‡
        """
        return self.encode_image_from_path(image_path)
    
    def encode_images_batch_from_paths(self, image_paths: List[str], batch_size: int = 32) -> Tuple[List[np.ndarray], List[str], List[Dict]]:
        """
        æ‰¹é‡ä»æœ¬åœ°è·¯å¾„ç¼–ç å›¾ç‰‡ - å¢å¼ºç‰ˆï¼Œè¿”å›è¯¦ç»†é”™è¯¯ä¿¡æ¯
        
        Returns:
            features: ç‰¹å¾å‘é‡åˆ—è¡¨
            valid_paths: æˆåŠŸå¤„ç†çš„å›¾ç‰‡è·¯å¾„åˆ—è¡¨  
            error_details: é”™è¯¯è¯¦æƒ…åˆ—è¡¨
        """
        features = []
        valid_paths = []
        error_details = []
        processed_count = 0
        
        for i in range(0, len(image_paths), batch_size):
            batch_paths = image_paths[i:i+batch_size]
            batch_images = []
            batch_valid_paths = []
            
            # é¢„å¤„ç†æ‰¹æ¬¡å›¾ç‰‡
            for path in batch_paths:
                try:
                    if not os.path.exists(path):
                        error_details.append({
                            'path': path,
                            'error': 'file_not_found',
                            'message': 'æ–‡ä»¶ä¸å­˜åœ¨'
                        })
                        continue
                    
                    # æ£€æŸ¥æ–‡ä»¶å¤§å°
                    try:
                        file_size = os.path.getsize(path)
                        if file_size == 0:
                            error_details.append({
                                'path': path,
                                'error': 'empty_file',
                                'message': 'æ–‡ä»¶ä¸ºç©º'
                            })
                            continue
                        
                        if file_size > 50 * 1024 * 1024:  # 50MB
                            error_details.append({
                                'path': path,
                                'error': 'file_too_large',
                                'message': f'æ–‡ä»¶è¿‡å¤§: {file_size/1024/1024:.1f}MB'
                            })
                            continue
                    except OSError as e:
                        error_details.append({
                            'path': path,
                            'error': 'file_access_error',
                            'message': f'æ–‡ä»¶è®¿é—®é”™è¯¯: {str(e)}'
                        })
                        continue
                    
                    # å°è¯•åŠ è½½å›¾ç‰‡
                    try:
                        image = Image.open(path).convert('RGB')
                        
                        # æ£€æŸ¥å›¾ç‰‡å°ºå¯¸
                        width, height = image.size
                        if width < 10 or height < 10:
                            error_details.append({
                                'path': path,
                                'error': 'invalid_dimensions',
                                'message': f'å›¾ç‰‡å°ºå¯¸è¿‡å°: {width}x{height}'
                            })
                            continue
                        
                        if width > 10000 or height > 10000:
                            error_details.append({
                                'path': path,
                                'error': 'dimensions_too_large', 
                                'message': f'å›¾ç‰‡å°ºå¯¸è¿‡å¤§: {width}x{height}'
                            })
                            continue
                        
                        # é¢„å¤„ç†å›¾ç‰‡
                        image_input = self.preprocess(image)
                        batch_images.append(image_input)
                        batch_valid_paths.append(path)
                        
                    except Exception as e:
                        error_details.append({
                            'path': path,
                            'error': 'image_processing_failed',
                            'message': f'å›¾ç‰‡å¤„ç†å¤±è´¥: {str(e)}'
                        })
                        continue
                        
                except Exception as e:
                    error_details.append({
                        'path': path,
                        'error': 'unexpected_error',
                        'message': f'æœªçŸ¥é”™è¯¯: {str(e)}'
                    })
                    continue
            
            if not batch_images:
                continue
            
            # æ‰¹é‡ç¼–ç 
            try:
                batch_tensor = torch.stack(batch_images).to(self.device)
                
                with torch.no_grad():
                    batch_features = self.model.encode_image(batch_tensor)
                    batch_features = batch_features / batch_features.norm(dim=-1, keepdim=True)
                
                # æ·»åŠ åˆ°ç»“æœåˆ—è¡¨
                for j, feature in enumerate(batch_features.cpu().numpy()):
                    features.append(feature)
                    valid_paths.append(batch_valid_paths[j])
                    processed_count += 1
                
                logger.info(f"æ‰¹æ¬¡ {i//batch_size + 1}/{(len(image_paths)-1)//batch_size + 1} å®Œæˆ - æˆåŠŸ: {processed_count}, å¤±è´¥: {len(error_details)}")
                
            except Exception as e:
                # å¦‚æœæ‰¹é‡ç¼–ç å¤±è´¥ï¼Œå°è¯•å•å¼ å¤„ç†
                logger.warning(f"æ‰¹é‡ç¼–ç å¤±è´¥ï¼Œå°è¯•å•å¼ å¤„ç†: {e}")
                
                for j, (path, image_input) in enumerate(zip(batch_valid_paths, batch_images)):
                    try:
                        single_tensor = image_input.unsqueeze(0).to(self.device)
                        
                        with torch.no_grad():
                            single_feature = self.model.encode_image(single_tensor)
                            single_feature = single_feature / single_feature.norm(dim=-1, keepdim=True)
                        
                        features.append(single_feature.cpu().numpy()[0])
                        valid_paths.append(path)
                        processed_count += 1
                        
                    except Exception as single_error:
                        error_details.append({
                            'path': path,
                            'error': 'encoding_failed',
                            'message': f'ç¼–ç å¤±è´¥: {str(single_error)}'
                        })
                        continue
        
        # ç»Ÿè®¡é”™è¯¯ç±»å‹
        error_stats = {}
        for error in error_details:
            error_type = error['error']
            error_stats[error_type] = error_stats.get(error_type, 0) + 1
        
        logger.info(f"æ‰¹é‡ç¼–ç å®Œæˆ - æ€»æˆåŠŸ: {processed_count}, æ€»å¤±è´¥: {len(error_details)}")
        if error_stats:
            logger.info(f"é”™è¯¯ç»Ÿè®¡: {error_stats}")
        
        return features, valid_paths, error_details
    
    def encode_text(self, text: str) -> Optional[np.ndarray]:
        """
        ç¼–ç æ–‡æœ¬
        Args:
            text: è¾“å…¥æ–‡æœ¬
        Returns:
            æ–‡æœ¬ç‰¹å¾å‘é‡
        """
        try:
            text_tokens = clip.tokenize([text]).to(self.device)
            
            with torch.no_grad():
                text_features = self.model.encode_text(text_tokens)
                text_features = text_features / text_features.norm(dim=-1, keepdim=True)
            
            return text_features.cpu().numpy()[0]
            
        except Exception as e:
            logger.error(f"æ–‡æœ¬ç¼–ç å¤±è´¥: {e}")
            return None
    
    def encode_image_from_pil(self, pil_image: Image.Image) -> Optional[np.ndarray]:
        """
        ä»PILå›¾ç‰‡ç¼–ç 
        Args:
            pil_image: PILå›¾ç‰‡å¯¹è±¡
        Returns:
            å›¾ç‰‡ç‰¹å¾å‘é‡
        """
        try:
            # ç¡®ä¿æ˜¯RGBæ ¼å¼
            if pil_image.mode != 'RGB':
                pil_image = pil_image.convert('RGB')
            
            # é¢„å¤„ç†å›¾ç‰‡
            image_input = self.preprocess(pil_image).unsqueeze(0).to(self.device)
            
            # ç¼–ç å›¾ç‰‡
            with torch.no_grad():
                image_features = self.model.encode_image(image_input)
                image_features = image_features / image_features.norm(dim=-1, keepdim=True)
                
            return image_features.cpu().numpy()[0]
            
        except Exception as e:
            logger.error(f"ä»PILå›¾ç‰‡ç¼–ç å¤±è´¥: {e}")
            return None

class MySQLDataProcessor:
    """MySQLæ•°æ®åº“å¤„ç†å™¨ - ä¼˜åŒ–ç‰ˆ"""
    
    def __init__(self):
        """åˆå§‹åŒ–MySQLæ•°æ®å¤„ç†å™¨"""
        # ç¡¬ç¼–ç æ•°æ®åº“é…ç½®
        self.db_config = {
            'host': '',
            'user': '',
            'password': '',
            'database': '',
            'port': 3306,
            'charset': 'utf8mb4'
        }
        
        # å›¾ç‰‡è·¯å¾„å‰ç¼€
        self.image_path_prefix = "/home/ai/"
        
        self.connection = None
        self.dataset_df = None
        self.available_tag_fields = []
        self.schema_info = None
        self.file_mapping = None  # å»¶è¿Ÿåˆå§‹åŒ–
        self._file_mapping_built = False  # æ·»åŠ æ ‡å¿—
        
        # æ·»åŠ æ•°æ®åº“çŠ¶æ€è¿½è¸ª
        self.last_known_count = None
        self.last_check_time = None
        self.status_file = "db_status.json"  # çŠ¶æ€æ–‡ä»¶
        
        # æµ‹è¯•è¿æ¥
        self._test_connection()
        
        logger.info(f"MySQLæ•°æ®å¤„ç†å™¨åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"åŸºç¡€å›¾ç‰‡è·¯å¾„: {self.image_path_prefix}")
    
    def _test_connection(self):
        """æµ‹è¯•æ•°æ®åº“è¿æ¥"""
        try:
            connection = pymysql.connect(**self.db_config)
            connection.close()
            logger.info(f"æ•°æ®åº“è¿æ¥æµ‹è¯•æˆåŠŸ - {self.db_config['host']}:{self.db_config['port']}")
        except Exception as e:
            logger.error(f"æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
            raise
    
    def _get_connection(self):
        """è·å–æ•°æ®åº“è¿æ¥"""
        if self.connection is None or not self.connection.open:
            self.connection = pymysql.connect(**self.db_config)
        return self.connection
    
    def _load_status(self):
        """åŠ è½½æ•°æ®åº“çŠ¶æ€"""
        try:
            if os.path.exists(self.status_file):
                with open(self.status_file, 'r', encoding='utf-8') as f:
                    status = json.load(f)
                    self.last_known_count = status.get('count')
                    self.last_check_time = status.get('check_time')
                    logger.info(f"åŠ è½½çŠ¶æ€: ä¸Šæ¬¡è®°å½• {self.last_known_count} æ¡æ•°æ®")
        except Exception as e:
            logger.warning(f"åŠ è½½çŠ¶æ€æ–‡ä»¶å¤±è´¥: {e}")
    
    def _save_status(self, count: int):
        """ä¿å­˜æ•°æ®åº“çŠ¶æ€"""
        try:
            status = {
                'count': count,
                'check_time': datetime.now().isoformat(),
                'database': self.db_config['database'],
                'table': 'work_copy428'
            }
            with open(self.status_file, 'w', encoding='utf-8') as f:
                json.dump(status, f, ensure_ascii=False, indent=2)
            
            self.last_known_count = count
            self.last_check_time = status['check_time']
            logger.info(f"ä¿å­˜çŠ¶æ€: {count} æ¡æ•°æ®")
        except Exception as e:
            logger.warning(f"ä¿å­˜çŠ¶æ€æ–‡ä»¶å¤±è´¥: {e}")
    
    def check_data_updates(self) -> Dict:
        """æ£€æŸ¥æ•°æ®åº“æ˜¯å¦æœ‰æ›´æ–°"""
        try:
            # åŠ è½½ä¸Šæ¬¡çŠ¶æ€
            self._load_status()
            
            connection = self._get_connection()
            
            # è·å–å½“å‰æ•°æ®åº“è®°å½•æ•°
            base_fields = ['id', 'image_url']
            tag_fields = []
            
            # æ£€æŸ¥å¯ç”¨å­—æ®µ
            schema_info = self.check_database_schema()
            if schema_info and schema_info['has_ai_tags']:
                tag_fields.append('ai_tags')
            if schema_info and schema_info['has_tags']:
                tag_fields.append('tags')
            
            # æ„å»ºæŸ¥è¯¢æ¡ä»¶
            where_conditions = ["image_url IS NOT NULL", "image_url != ''"]
            
            if tag_fields:
                tag_conditions = []
                for field in tag_fields:
                    tag_conditions.append(f"({field} IS NOT NULL AND {field} != '')")
                where_conditions.append(f"({' OR '.join(tag_conditions)})")
            
            where_clause = ' AND '.join(where_conditions)
            
            # è·å–å½“å‰æ€»æ•°
            count_sql = f"""
                SELECT COUNT(*) as total_count
                FROM work_copy428 
                WHERE {where_clause}
            """
            
            with connection.cursor() as cursor:
                cursor.execute(count_sql)
                current_count = cursor.fetchone()[0]
            
            # æ¯”è¾ƒæ•°æ®å˜åŒ–
            if self.last_known_count is None:
                # é¦–æ¬¡æ£€æŸ¥
                change_type = 'initial'
                change_count = current_count
                message = f"é¦–æ¬¡æ£€æµ‹åˆ°æ•°æ®åº“ä¸­æœ‰ {current_count:,} æ¡è®°å½•"
            elif current_count > self.last_known_count:
                # æ•°æ®å¢åŠ 
                change_type = 'increased'
                change_count = current_count - self.last_known_count
                message = f"æ•°æ®åº“æ›´æ–°ï¼šæ–°å¢ {change_count:,} æ¡è®°å½• ({self.last_known_count:,} â†’ {current_count:,})"
            elif current_count < self.last_known_count:
                # æ•°æ®å‡å°‘
                change_type = 'decreased'
                change_count = self.last_known_count - current_count
                message = f"æ•°æ®åº“æ›´æ–°ï¼šå‡å°‘ {change_count:,} æ¡è®°å½• ({self.last_known_count:,} â†’ {current_count:,})"
            else:
                # æ— å˜åŒ–
                change_type = 'no_change'
                change_count = 0
                message = f"æ•°æ®åº“æ— å˜åŒ–ï¼Œä¿æŒ {current_count:,} æ¡è®°å½•"
            
            # è·å–æœ€æ–°è®°å½•çš„æ—¶é—´æˆ³
            try:
                timestamp_sql = f"""
                    SELECT MAX(id) as latest_id
                    FROM work_copy428 
                    WHERE {where_clause}
                """
                with connection.cursor() as cursor:
                    cursor.execute(timestamp_sql)
                    latest_id = cursor.fetchone()[0]
            except:
                latest_id = None
            
            return {
                'has_updates': change_type != 'no_change',
                'change_type': change_type,
                'change_count': change_count,
                'current_count': current_count,
                'last_known_count': self.last_known_count,
                'latest_id': latest_id,
                'message': message,
                'check_time': datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"æ£€æŸ¥æ•°æ®æ›´æ–°å¤±è´¥: {e}")
            return {
                'has_updates': False,
                'change_type': 'error',
                'change_count': 0,
                'current_count': 0,
                'last_known_count': self.last_known_count,
                'message': f"æ£€æŸ¥å¤±è´¥: {e}",
                'error': str(e)
            }
    
    def check_database_schema(self):
        """æ£€æŸ¥æ•°æ®åº“è¡¨ç»“æ„"""
        try:
            connection = self._get_connection()
            
            # æ£€æŸ¥è¡¨ç»“æ„
            sql = "DESCRIBE work_copy428"
            
            with connection.cursor() as cursor:
                cursor.execute(sql)
                columns = cursor.fetchall()
            
            logger.info("æ•°æ®åº“è¡¨å­—æ®µç»“æ„:")
            available_columns = []
            for column in columns:
                field_name = column[0]
                field_type = column[1]
                available_columns.append(field_name)
                logger.info(f"  {field_name}: {field_type}")
            
            # æ£€æŸ¥å…³é”®å­—æ®µ
            has_ai_tags = 'ai_tags' in available_columns
            has_tags = 'tags' in available_columns
            
            logger.info(f"å…³é”®å­—æ®µæ£€æŸ¥:")
            logger.info(f"  ai_tagså­—æ®µ: {'âœ… å­˜åœ¨' if has_ai_tags else 'âŒ ä¸å­˜åœ¨'}")
            logger.info(f"  tagså­—æ®µ: {'âœ… å­˜åœ¨' if has_tags else 'âŒ ä¸å­˜åœ¨'}")
            
            return {
                'available_columns': available_columns,
                'has_ai_tags': has_ai_tags,
                'has_tags': has_tags
            }
            
        except Exception as e:
            logger.error(f"æ£€æŸ¥æ•°æ®åº“ç»“æ„å¤±è´¥: {e}")
            return None
    
    def load_data(self, limit: int = 183247, offset: int = 0, save_status: bool = True):
        """
        ä»æ•°æ®åº“åŠ è½½æ•°æ® - åŠ¨æ€é€‚é…å­—æ®µ
        Args:
            limit: é™åˆ¶åŠ è½½çš„è®°å½•æ•° (é»˜è®¤183247)
            offset: åç§»é‡
            save_status: æ˜¯å¦ä¿å­˜çŠ¶æ€
        """
        try:
            # å…ˆæ£€æŸ¥æ•°æ®åº“ç»“æ„
            schema_info = self.check_database_schema()
            if not schema_info:
                raise Exception("æ— æ³•è·å–æ•°æ®åº“ç»“æ„ä¿¡æ¯")
            
            connection = self._get_connection()
            
            # æ ¹æ®å­˜åœ¨çš„å­—æ®µåŠ¨æ€æ„å»ºæŸ¥è¯¢
            base_fields = ['id', 'image_url']
            tag_fields = []
            
            if schema_info['has_ai_tags']:
                tag_fields.append('ai_tags')
            if schema_info['has_tags']:
                tag_fields.append('tags')
            
            if not tag_fields:
                raise Exception("æ•°æ®åº“ä¸­æ²¡æœ‰æ‰¾åˆ°ai_tagsæˆ–tagså­—æ®µ")
            
            all_fields = base_fields + tag_fields
            fields_str = ', '.join(all_fields)
            
            # æ„å»ºWHEREæ¡ä»¶ - åŠ¨æ€é€‚é…
            where_conditions = ["image_url IS NOT NULL", "image_url != ''"]
            
            # æ·»åŠ æ ‡ç­¾å­—æ®µçš„æ¡ä»¶
            tag_conditions = []
            for field in tag_fields:
                tag_conditions.append(f"({field} IS NOT NULL AND {field} != '')")
            
            if tag_conditions:
                where_conditions.append(f"({' OR '.join(tag_conditions)})")
            
            where_clause = ' AND '.join(where_conditions)
            
            sql = f"""
                SELECT {fields_str}
                FROM work_copy428 
                WHERE {where_clause}
                ORDER BY id
                LIMIT %s OFFSET %s
            """
            
            logger.info(f"åŠ¨æ€SQLæŸ¥è¯¢: {sql}")
            logger.info(f"å¯ç”¨å­—æ®µ: {all_fields}")
            logger.info(f"æ‰§è¡ŒæŸ¥è¯¢: é™åˆ¶ {limit} æ¡è®°å½•ï¼Œåç§» {offset}")
            
            # æ‰§è¡ŒæŸ¥è¯¢
            with connection.cursor() as cursor:
                cursor.execute(sql, (limit, offset))
                results = cursor.fetchall()
            
            # è½¬æ¢ä¸ºDataFrame
            self.dataset_df = pd.DataFrame(results, columns=all_fields)
            
            # ä¿å­˜å­—æ®µä¿¡æ¯
            self.available_tag_fields = tag_fields
            self.schema_info = schema_info
            
            # æ•°æ®æ¸…æ´—å’Œå¤„ç†
            self._clean_data()
            
            # ä¿å­˜çŠ¶æ€ï¼ˆå¦‚æœéœ€è¦ï¼‰
            if save_status:
                current_count = len(self.dataset_df)
                self._save_status(current_count)
            
            logger.info(f"æˆåŠŸåŠ è½½ {len(self.dataset_df)} æ¡æ•°æ®")
            
        except Exception as e:
            logger.error(f"æ•°æ®åŠ è½½å¤±è´¥: {e}")
            raise
    
    def _build_file_mapping_once(self):
        """åªæ„å»ºä¸€æ¬¡æ–‡ä»¶æ˜ å°„è¡¨"""
        if self._file_mapping_built and self.file_mapping is not None:
            logger.info(f"æ–‡ä»¶æ˜ å°„è¡¨å·²å­˜åœ¨ï¼ŒåŒ…å« {len(self.file_mapping)} ä¸ªæ–‡ä»¶")
            return
        
        logger.info("é¦–æ¬¡æ„å»ºæ–‡ä»¶æ˜ å°„è¡¨...")
        self.file_mapping = {}
        scraper_base = f'{self.image_path_prefix}scraper_data/'
        
        if os.path.exists(scraper_base):
            for brand_dir in os.listdir(scraper_base):
                brand_path = os.path.join(scraper_base, brand_dir)
                if os.path.isdir(brand_path):
                    try:
                        for filename in os.listdir(brand_path):
                            if filename.endswith(('.png', '.jpg', '.jpeg')):
                                full_path = os.path.join(brand_path, filename)
                                self.file_mapping[filename] = full_path
                    except Exception as e:
                        logger.warning(f"è¯»å–ç›®å½• {brand_dir} å¤±è´¥: {e}")
        
        self._file_mapping_built = True
        logger.info(f"æ–‡ä»¶æ˜ å°„è¡¨æ„å»ºå®Œæˆï¼Œæ˜ å°„äº† {len(self.file_mapping)} ä¸ªæ–‡ä»¶")
    
    def _clean_data(self):
        """æ¸…æ´—å’Œå¤„ç†æ•°æ® - ä¼˜åŒ–ç‰ˆæœ¬"""
        try:
            initial_count = len(self.dataset_df)
            self.dataset_df = self.dataset_df.dropna(subset=['image_url'])
            
            # åŠ¨æ€è¿‡æ»¤ç©ºæ ‡ç­¾è®°å½•
            if hasattr(self, 'available_tag_fields'):
                tag_fields = self.available_tag_fields
            else:
                # å›é€€é€»è¾‘
                tag_fields = []
                if 'ai_tags' in self.dataset_df.columns:
                    tag_fields.append('ai_tags')
                if 'tags' in self.dataset_df.columns:
                    tag_fields.append('tags')
            
            logger.info(f"å¯ç”¨æ ‡ç­¾å­—æ®µ: {tag_fields}")
            
            # è¿‡æ»¤æ‰æ‰€æœ‰æ ‡ç­¾å­—æ®µéƒ½ä¸ºç©ºçš„è®°å½•
            if tag_fields:
                mask = pd.Series([False] * len(self.dataset_df), index=self.dataset_df.index)
                for field in tag_fields:
                    field_mask = (
                        self.dataset_df[field].notna() & 
                        (self.dataset_df[field] != '') &
                        (self.dataset_df[field] != 'null')
                    )
                    mask = mask | field_mask
                
                self.dataset_df = self.dataset_df[mask]
                logger.info(f"æ ‡ç­¾è¿‡æ»¤åå‰©ä½™: {len(self.dataset_df)} æ¡è®°å½•")
            
            # åªåœ¨éœ€è¦æ—¶æ„å»ºæ–‡ä»¶æ˜ å°„è¡¨
            self._build_file_mapping_once()
            
            # å¤„ç†å›¾ç‰‡è·¯å¾„ï¼ˆä½¿ç”¨å·²æœ‰çš„æ˜ å°„è¡¨ï¼‰
            def process_image_path(image_url):
                if pd.isna(image_url) or image_url == '':
                    return ''
                
                clean_url = str(image_url).strip()
                filename = os.path.basename(clean_url)
                
                if filename in self.file_mapping:
                    return self.file_mapping[filename]
                
                if clean_url.startswith('/scraper_data/'):
                    return clean_url.replace('/scraper_data/', f'{self.image_path_prefix}scraper_data/')
                else:
                    return os.path.join(self.image_path_prefix, clean_url.lstrip('/'))
            
            self.dataset_df['full_image_path'] = self.dataset_df['image_url'].apply(process_image_path)
            
            # åŠ¨æ€å¤„ç†å’Œåˆå¹¶æ ‡ç­¾
            def process_and_merge_tags(row):
                """åŠ¨æ€å¤„ç†å¹¶åˆå¹¶æ‰€æœ‰å¯ç”¨çš„æ ‡ç­¾å­—æ®µ"""
                combined_tags = []
                
                # éå†æ‰€æœ‰å¯ç”¨çš„æ ‡ç­¾å­—æ®µ
                for field in tag_fields:
                    if field in row and pd.notna(row[field]) and str(row[field]).strip():
                        field_value = str(row[field]).strip()
                        
                        # è·³è¿‡æ˜æ˜¾çš„ç©ºå€¼
                        if field_value.lower() in ['null', 'none', '']:
                            continue
                        
                        try:
                            # å°è¯•è§£æJSONæ ¼å¼
                            if field_value.startswith('[') or field_value.startswith('{'):
                                parsed = json.loads(field_value)
                                if isinstance(parsed, list):
                                    parsed_tags = ', '.join(str(tag) for tag in parsed if tag)
                                elif isinstance(parsed, dict):
                                    parsed_tags = ', '.join(f"{k}: {v}" for k, v in parsed.items() if v)
                                else:
                                    parsed_tags = str(parsed)
                                
                                if parsed_tags:
                                    combined_tags.append(parsed_tags)
                            else:
                                combined_tags.append(field_value)
                                
                        except (json.JSONDecodeError, Exception):
                            # å¦‚æœä¸æ˜¯JSONï¼Œç›´æ¥ä½¿ç”¨åŸå€¼
                            combined_tags.append(field_value)
                
                return ' | '.join(combined_tags) if combined_tags else 'æ— æ ‡ç­¾ä¿¡æ¯'
            
            # åº”ç”¨æ ‡ç­¾åˆå¹¶å¤„ç†
            self.dataset_df['processed_tags'] = self.dataset_df.apply(process_and_merge_tags, axis=1)
            
            # æ·»åŠ æ–‡ä»¶åä¿¡æ¯
            self.dataset_df['filename'] = self.dataset_df['image_url'].apply(
                lambda x: os.path.basename(x) if x else ''
            )
            
            # éªŒè¯å›¾ç‰‡è·¯å¾„æœ‰æ•ˆæ€§
            def path_exists_check(path):
                try:
                    return os.path.exists(path) if path else False
                except:
                    return False
            
            self.dataset_df['file_exists'] = self.dataset_df['full_image_path'].apply(path_exists_check)
            valid_count = self.dataset_df['file_exists'].sum()
            
            # ç»Ÿè®¡æ ‡ç­¾å­—æ®µ
            tag_stats = {}
            for field in tag_fields:
                if field in self.dataset_df.columns:
                    non_empty = (
                        self.dataset_df[field].notna() & 
                        (self.dataset_df[field] != '') &
                        (self.dataset_df[field] != 'null')
                    ).sum()
                    tag_stats[field] = int(non_empty)
            
            logger.info(f"æ ‡ç­¾åˆå¹¶å’Œè·¯å¾„å¤„ç†å®Œæˆ:")
            logger.info(f"  æ€»è®°å½•: {len(self.dataset_df)}")
            logger.info(f"  æ–‡ä»¶åŒ¹é…æˆåŠŸ: {valid_count}")
            logger.info(f"  æ ‡ç­¾ç»Ÿè®¡: {tag_stats}")
            
            # æ˜¾ç¤ºæ ·æœ¬æ•°æ®
            sample_data = self.dataset_df[['id'] + tag_fields + ['processed_tags']].head(3)
            logger.info(f"æ ‡ç­¾æ ·æœ¬æ•°æ®:")
            for idx, row in sample_data.iterrows():
                logger.info(f"  ID {row['id']}:")
                for field in tag_fields:
                    value = str(row[field])[:100] if pd.notna(row[field]) else 'null'
                    logger.info(f"    {field}: {value}")
                processed = str(row['processed_tags'])[:100]
                logger.info(f"    processed_tags: {processed}")
            
            cleaned_count = len(self.dataset_df)
            logger.info(f"æ•°æ®æ¸…æ´—å®Œæˆ: {initial_count} -> {cleaned_count}")
            
        except Exception as e:
            logger.error(f"æ•°æ®æ¸…æ´—å¤±è´¥: {e}")
            raise
    
    def ensure_data_loaded(self, limit: int = 183247):
        """ç¡®ä¿æ•°æ®å·²åŠ è½½ï¼ˆå»¶è¿ŸåŠ è½½ï¼‰"""
        if self.dataset_df is None:
            logger.info("å»¶è¿ŸåŠ è½½æ•°æ®åº“æ•°æ®...")
            self.load_data(limit=limit)
        else:
            logger.info(f"æ•°æ®å·²åŠ è½½ï¼ŒåŒ…å« {len(self.dataset_df)} æ¡è®°å½•")
    
    def get_dataset_info(self) -> Dict:
        """è·å–æ•°æ®é›†ä¿¡æ¯"""
        if self.dataset_df is None or len(self.dataset_df) == 0:
            return {"total_images": 0}
        
        # ç»Ÿè®¡æ–‡ä»¶å­˜åœ¨æƒ…å†µ
        existing_files = self.dataset_df['file_exists'].sum()
        missing_files = len(self.dataset_df) - existing_files
        
        # æ ‡ç­¾ç»Ÿè®¡
        tag_stats = {}
        if hasattr(self, 'available_tag_fields'):
            for field in self.available_tag_fields:
                if field in self.dataset_df.columns:
                    non_empty = (
                        self.dataset_df[field].notna() & 
                        (self.dataset_df[field] != '') &
                        (self.dataset_df[field] != 'null')
                    ).sum()
                    tag_stats[field] = int(non_empty)
        
        return {
            "total_records": len(self.dataset_df),
            "existing_files": int(existing_files),
            "missing_files": int(missing_files),
            "data_source": f"MySQL Database - {self.db_config['host']}",
            "database": self.db_config['database'],
            "table": "work_copy428", 
            "path_prefix": self.image_path_prefix,
            "columns": list(self.dataset_df.columns),
            "available_tag_fields": self.available_tag_fields,
            "tag_stats": tag_stats,
            "sample_data": self.dataset_df[['id', 'image_url', 'full_image_path', 'processed_tags', 'file_exists']].head(3).to_dict('records')
        }

    def close_connection(self):
        """å…³é—­æ•°æ®åº“è¿æ¥"""
        if self.connection and self.connection.open:
            self.connection.close()
            logger.info("æ•°æ®åº“è¿æ¥å·²å…³é—­")

class ChromaDBManager:
    """ChromaDBæ•°æ®åº“ç®¡ç†å™¨ - å¢å¼ºç‰ˆ"""
    
    def __init__(self, host: str = "localhost", port: int = 6600, 
                 collection_name: str = "local_image_collection",
                 fallback_local_path: str = "./local_chromadb"):
        """åˆå§‹åŒ–ChromaDBç®¡ç†å™¨"""
        self.host = host
        self.port = port
        self.collection_name = collection_name
        self.fallback_local_path = fallback_local_path
        self.client = None
        self.collection = None
        self.is_local_mode = False
        
        self._connect()
        self._verify_persistence()
    
    def _connect(self):
        """å»ºç«‹æ•°æ®åº“è¿æ¥"""
        try:
            # å°è¯•è¿æ¥Dockerä¸­çš„ChromaDB
            self.client = chromadb.HttpClient(host=self.host, port=self.port)
            # æµ‹è¯•è¿æ¥
            self.client.heartbeat()
            self.is_local_mode = False
            logger.info(f"è¿æ¥åˆ°Docker ChromaDB: http://{self.host}:{self.port}")
            
        except Exception as e:
            logger.warning(f"Dockerè¿æ¥å¤±è´¥: {e}")
            logger.info("å›é€€åˆ°æœ¬åœ°æ–‡ä»¶æ¨¡å¼...")
            
            try:
                Path(self.fallback_local_path).mkdir(parents=True, exist_ok=True)
                self.client = chromadb.PersistentClient(path=self.fallback_local_path)
                self.is_local_mode = True
                logger.info(f"ä½¿ç”¨æœ¬åœ°æ¨¡å¼: {self.fallback_local_path}")
            except Exception as e2:
                logger.error(f"æœ¬åœ°æ¨¡å¼å¤±è´¥: {e2}")
                raise ConnectionError("æ— æ³•è¿æ¥åˆ°ChromaDB")
        
        # åˆ›å»ºæˆ–è·å–é›†åˆ
        self._setup_collection()
    
    def _setup_collection(self):
        """è®¾ç½®é›†åˆ"""
        try:
            self.collection = self.client.get_collection(self.collection_name)
            logger.info(f"åŠ è½½å·²å­˜åœ¨é›†åˆ: {self.collection_name}")
        except:
            self.collection = self.client.create_collection(
                name=self.collection_name,
                metadata={"description": "CLIPæœ¬åœ°å›¾ç‰‡å‘é‡é›†åˆ"}
            )
            logger.info(f"åˆ›å»ºæ–°é›†åˆ: {self.collection_name}")
    
    def _verify_persistence(self):
        """éªŒè¯æ•°æ®æŒä¹…åŒ–"""
        try:
            # æ£€æŸ¥ç°æœ‰æ•°æ®
            count = self.collection.count()
            
            if count > 0:
                logger.info(f"âœ… æ£€æµ‹åˆ°æŒä¹…åŒ–æ•°æ®: {count} æ¡è®°å½•")
                
                # éªŒè¯æ•°æ®å®Œæ•´æ€§
                sample = self.collection.get(limit=1, include=['metadatas'])
                if sample and sample['ids']:
                    logger.info(f"âœ… æ•°æ®è®¿é—®æ­£å¸¸")
                else:
                    logger.warning("âš ï¸ æ•°æ®å¯èƒ½æŸå")
            else:
                logger.info("ğŸ’¾ ç©ºé›†åˆï¼Œç­‰å¾…æ•°æ®å†™å…¥")
                
        except Exception as e:
            logger.error(f"âŒ æŒä¹…åŒ–éªŒè¯å¤±è´¥: {e}")
    
    def add_images(self, embeddings: List[List[float]], metadatas: List[Dict], 
                   documents: List[str], ids: List[str], batch_size: int = 5000):
        """
        åˆ†æ‰¹æ·»åŠ å›¾ç‰‡æ•°æ® - å¢å¼ºç‰ˆæœ¬
        """
        total_items = len(embeddings)
        
        if total_items == 0:
            logger.warning("æ²¡æœ‰æ•°æ®éœ€è¦æ·»åŠ ")
            return
        
        logger.info(f"å¼€å§‹åˆ†æ‰¹æ’å…¥ {total_items} æ¡æ•°æ®ï¼Œæ‰¹æ¬¡å¤§å°: {batch_size}")
        
        successful_batches = 0
        failed_batches = 0
        total_inserted = 0
        
        try:
            for i in range(0, total_items, batch_size):
                end_idx = min(i + batch_size, total_items)
                
                batch_embeddings = embeddings[i:end_idx]
                batch_metadatas = metadatas[i:end_idx]
                batch_documents = documents[i:end_idx]
                batch_ids = ids[i:end_idx]
                
                batch_num = i // batch_size + 1
                total_batches = (total_items + batch_size - 1) // batch_size
                
                logger.info(f"æ’å…¥æ‰¹æ¬¡ {batch_num}/{total_batches}: {len(batch_embeddings)} æ¡è®°å½•")
                
                try:
                    self.collection.add(
                        embeddings=batch_embeddings,
                        metadatas=batch_metadatas,
                        documents=batch_documents,
                        ids=batch_ids
                    )
                    
                    # ç«‹å³éªŒè¯å†™å…¥
                    new_count = self.collection.count()
                    logger.info(f"âœ… æ‰¹æ¬¡ {batch_num} æ’å…¥æˆåŠŸ, å½“å‰æ€»æ•°: {new_count}")
                    
                    successful_batches += 1
                    total_inserted += len(batch_embeddings)
                    
                except Exception as e:
                    logger.error(f"âŒ æ‰¹æ¬¡ {batch_num} æ’å…¥å¤±è´¥: {e}")
                    failed_batches += 1
                    continue
            
            # æœ€ç»ˆéªŒè¯
            final_count = self.collection.count()
            logger.info(f"ğŸ“Š æœ€ç»ˆéªŒè¯: ChromaDBä¸­å…±æœ‰ {final_count} æ¡è®°å½•")
            
            logger.info(f"ğŸ“Š åˆ†æ‰¹æ’å…¥å®Œæˆ:")
            logger.info(f"   æ€»æ‰¹æ¬¡: {successful_batches + failed_batches}")
            logger.info(f"   æˆåŠŸæ‰¹æ¬¡: {successful_batches}")
            logger.info(f"   å¤±è´¥æ‰¹æ¬¡: {failed_batches}")
            logger.info(f"   æˆåŠŸæ’å…¥: {total_inserted} æ¡è®°å½•")
            
            if failed_batches > 0:
                logger.warning(f"æœ‰ {failed_batches} ä¸ªæ‰¹æ¬¡æ’å…¥å¤±è´¥")
            
        except Exception as e:
            logger.error(f"åˆ†æ‰¹æ’å…¥è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
            raise
    
    def search_similar_images(self, query_vector: List[float], top_k: int = 10,
                            where: Optional[Dict] = None) -> Dict:
        """
        æœç´¢ç›¸ä¼¼å›¾ç‰‡
        Args:
            query_vector: æŸ¥è¯¢å‘é‡
            top_k: è¿”å›ç»“æœæ•°é‡
            where: è¿‡æ»¤æ¡ä»¶
        Returns:
            æœç´¢ç»“æœ
        """
        try:
            results = self.collection.query(
                query_embeddings=[query_vector],
                n_results=top_k,
                where=where,
                include=['metadatas', 'documents', 'distances']
            )
            return results
        except Exception as e:
            logger.error(f"ç›¸ä¼¼å›¾ç‰‡æœç´¢å¤±è´¥: {e}")
            return {}
    
    def get_collection_info(self) -> Dict:
        """è·å–é›†åˆä¿¡æ¯"""
        try:
            count = self.collection.count()
            return {
                "name": self.collection_name,
                "count": count,
                "mode": "local" if self.is_local_mode else "docker"
            }
        except Exception as e:
            logger.error(f"è·å–é›†åˆä¿¡æ¯å¤±è´¥: {e}")
            return {}
    
    def reset_collection(self):
        """é‡ç½®é›†åˆ"""
        try:
            self.client.delete_collection(self.collection_name)
            self.collection = self.client.create_collection(
                name=self.collection_name,
                metadata={"description": "CLIPæœ¬åœ°å›¾ç‰‡å‘é‡é›†åˆ"}
            )
            logger.info("é›†åˆå·²é‡ç½®")
        except Exception as e:
            logger.error(f"é‡ç½®é›†åˆå¤±è´¥: {e}")
            
    def get_all_existing_ids(self) -> set:
        """è·å–ChromaDBä¸­æ‰€æœ‰å·²å­˜åœ¨çš„ID - ä¿®å¤ç‰ˆ"""
        try:
            existing_ids = set()
            batch_size = 5000  # å‡å°æ‰¹æ¬¡å¤§å°
            
            # å…ˆè·å–æ€»æ•°
            total_count = self.collection.count()
            logger.info(f"ChromaDBæ€»è®°å½•æ•°: {total_count}")
            
            if total_count == 0:
                return set()
            
            # åˆ†æ‰¹è·å–æ‰€æœ‰æ•°æ®
            processed = 0
            while processed < total_count:
                try:
                    # è·å–è¿™ä¸€æ‰¹æ•°æ®
                    results = self.collection.get(
                        limit=batch_size,
                        offset=processed,
                        include=['metadatas']
                    )
                    
                    if not results or not results.get('metadatas'):
                        break
                    
                    batch_count = len(results['metadatas'])
                    logger.info(f"è·å–æ‰¹æ¬¡: {processed}-{processed + batch_count}/{total_count}")
                    
                    # æå–ID
                    for metadata in results['metadatas']:
                        if 'id' in metadata:
                            try:
                                existing_ids.add(int(metadata['id']))
                            except (ValueError, TypeError):
                                logger.warning(f"æ— æ•ˆID: {metadata.get('id')}")
                    
                    processed += batch_count
                    
                    # å¦‚æœè¿”å›çš„æ•°æ®å°‘äºbatch_sizeï¼Œè¯´æ˜å·²ç»è·å–å®Œäº†
                    if batch_count < batch_size:
                        break
                        
                except Exception as e:
                    logger.error(f"è·å–æ‰¹æ¬¡ {processed} å¤±è´¥: {e}")
                    processed += batch_size  # è·³è¿‡è¿™ä¸ªæ‰¹æ¬¡
                    continue
            
            logger.info(f"æˆåŠŸè·å– {len(existing_ids)} ä¸ªå·²å­˜åœ¨çš„ID")
            return existing_ids
            
        except Exception as e:
            logger.error(f"è·å–å·²å­˜åœ¨IDå¤±è´¥: {e}")
            return set()

class DatabaseImageRetrievalSystem:
    """åŸºäºæ•°æ®åº“çš„æœ¬åœ°å›¾ç‰‡æ£€ç´¢ç³»ç»Ÿ"""
    
    def __init__(self, 
                 clip_model: str = "ViT-B/32",
                 chromadb_host: str = "localhost", 
                 chromadb_port: int = 6600,
                 collection_name: str = "local_db_image_collection"):
        """åˆå§‹åŒ–æ•°æ®åº“å›¾ç‰‡æ£€ç´¢ç³»ç»Ÿ"""
        logger.info("åˆå§‹åŒ–æœ¬åœ°å›¾ç‰‡æ£€ç´¢ç³»ç»Ÿ...")
        
        # åˆå§‹åŒ–å„ä¸ªç»„ä»¶
        self.clip_encoder = CLIPImageEncoder(clip_model)
        self.chromadb = ChromaDBManager(chromadb_host, chromadb_port, collection_name)
        self.db_processor = MySQLDataProcessor()
        self.tag_keywords = {
            "è‰²å½©": ["å•è‰²ç³»", "å¯¹æ¯”è‰²", "é»‘ç™½", "é‡‘å±è‰²", "å“‘å…‰è‰²", "é²œè‰³è‰²å½©", "æŸ”å’Œè‰²å½©", "å¤å¤è‰²å½©", "æ¢¦å¹»è‰²å½©"],
            "è‰²è°ƒ":["å†·è‰²è°ƒ", "æš–è‰²è°ƒ", "ä¸­æ€§è‰²è°ƒ", "é«˜å¯¹æ¯”åº¦", "ä½å¯¹æ¯”åº¦", "æ˜äº®è‰²è°ƒ", "æš—é»‘è‰²è°ƒ", "é»„æ˜è‰²è°ƒ", "è¤ªè‰²æ•ˆæœ", "å¤œæ™¯è‰²è°ƒ", "é¥±å’Œè‰²è°ƒ"],
            "å…‰çº¿":["è‡ªç„¶å…‰çº¿", "äººå·¥å…‰çº¿", "æŸ”å’Œå…‰çº¿", "å¼ºçƒˆå…‰çº¿", "ä¾§å…‰", "é€†å…‰", "é¡ºå…‰", "æ¼«å°„å…‰", "å…‰å½±å¯¹æ¯”", "é»„é‡‘æ—¶åˆ»å…‰çº¿", "è“è°ƒæ—¶åˆ»å…‰çº¿", "å¤œæ™šå…‰çº¿"],
            "æ„å›¾":["ä¸­å¿ƒæ„å›¾", "å¯¹ç§°æ„å›¾", "ä¸‰åˆ†æ³•æ„å›¾", "å‰æ™¯æ¡†æ¶", "å¼•å¯¼çº¿æ„å›¾", "é‡å¤å…ƒç´ ", "è´Ÿç©ºé—´æ„å›¾", "å¯¹è§’çº¿æ„å›¾", "å±‚æ¬¡æ„å›¾", "æœ€å°åŒ–æ„å›¾", "é»„é‡‘æ¯”ä¾‹æ„å›¾"],
            "è´¨æ„Ÿ":["é‡‘å±è´¨æ„Ÿ", "å…‰æ»‘è´¨æ„Ÿ", "å“‘å…‰è´¨æ„Ÿ", "ç²—ç³™è´¨æ„Ÿ", "åå…‰è´¨æ„Ÿ", "çš®é©è´¨æ„Ÿ", "ç§‘æŠ€è´¨æ„Ÿ", "å¥¢åè´¨æ„Ÿ", "å¤å¤è´¨æ„Ÿ", "è‡ªç„¶è´¨æ„Ÿ"],
            "äººè½¦äº’åŠ¨":["ç”Ÿæ´»", "å®¶åº­", "ä¼‘é—²", "è¡—æ‹", "åŸå¸‚", "é£æ™¯", "å»ºç­‘", "é©¾é©¶åœºæ™¯", "å®¶åº­å‡ºæ¸¸", "å•†åŠ¡å‡ºè¡Œ", "ä¼‘é—²æ—…è¡Œ", "æˆ·å¤–æ¢é™©", "åŸå¸‚é€šå‹¤", "ç¤¾äº¤èšä¼š", "å±•ç¤ºåœºæ™¯", "è¯•é©¾åœºæ™¯", "å„¿ç«¥äº’åŠ¨", "å® ç‰©äº’åŠ¨", "æƒ…ä¾£åœºæ™¯"],
            "ç”»é¢é£æ ¼":["æ‘„å½±", "CG", "æç®€", "å•†ä¸šé£æ ¼", "ç”Ÿæ´»çºªå®", "å¤å¤é£æ ¼", "æœªæ¥é£æ ¼", "è‰ºæœ¯åˆ›æ„", "å·¥ä¸šé£æ ¼", "è¿åŠ¨é£æ ¼", "å¥¢åé£æ ¼", "ç§‘æŠ€é£æ ¼", "ç”µå½±æ„Ÿ"],
            "æ‹æ‘„è§†è§’":["ç‰¹å†™","æ­£é¢è§†è§’", "ä¾§é¢è§†è§’", "45åº¦è§’", "åè§†å›¾", "ä¿¯è§†å›¾", "ä»°è§†è§’", "è½¦å†…è§†è§’", "å…¨æ™¯è§†è§’", "é¸Ÿç°è§†è§’",],
            "è½¦å‹": ["è½¿è½¦", "SUV", "è¶Šé‡", "æˆ¿è½¦", "MPV", "ç´§å‡‘å‹è½¿è½¦", "ä¸­å‹è½¿è½¦", "è±ªåè½¿è½¦", "è·‘è½¦", "çš®å¡", "å¤å…¸è½¦", "ç”µåŠ¨è½¦"],
        }
        
        # æ£€æŸ¥ç°æœ‰ç´¢å¼•çŠ¶æ€
        try:
            collection_info = self.chromadb.get_collection_info()
            existing_count = collection_info.get('count', 0)
            self.is_indexed = existing_count > 0
            
            if self.is_indexed:
                logger.info(f"æ£€æµ‹åˆ°ç°æœ‰ç´¢å¼•: {existing_count} æ¡è®°å½•")
            else:
                logger.info("æœªæ£€æµ‹åˆ°ç°æœ‰ç´¢å¼•")
                
        except Exception as e:
            logger.warning(f"æ£€æŸ¥ç´¢å¼•çŠ¶æ€å¤±è´¥: {e}")
            self.is_indexed = False
        
        logger.info("æœ¬åœ°å›¾ç‰‡æ£€ç´¢ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
    
    def build_index(self, batch_size: int = 32, force_rebuild: bool = False, 
                   limit: int = 183247, only_existing_files: bool = True,
                   chromadb_batch_size: int = 4000):
        """æ„å»ºå›¾ç‰‡ç´¢å¼• - å¢å¼ºç‰ˆ"""
        # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡å»º
        collection_info = self.chromadb.get_collection_info()
        if not force_rebuild and collection_info.get('count', 0) > 0:
            logger.info(f"æ£€æµ‹åˆ°å·²å­˜åœ¨ {collection_info['count']} æ¡æ•°æ®ï¼Œè·³è¿‡æ„å»º")
            self.is_indexed = True
            return
        
        if force_rebuild:
            logger.info("å¼ºåˆ¶é‡å»ºç´¢å¼•...")
            self.chromadb.reset_collection()
        
        logger.info(f"å¼€å§‹æ„å»ºå›¾ç‰‡ç´¢å¼• - é™åˆ¶: {limit} å¼ å›¾ç‰‡...")
        
        # åŠ è½½æ•°æ®
        if self.db_processor.dataset_df is None:
            self.db_processor.load_data(limit=limit)
        
        # è·å–æ•°æ®
        dataset_df = self.db_processor.dataset_df
        if len(dataset_df) == 0:
            logger.error("æ²¡æœ‰å¯ç”¨çš„æ•°æ®")
            return
        
        # è·å–å›¾ç‰‡è·¯å¾„
        if only_existing_files:
            valid_df = dataset_df[dataset_df['file_exists'] == True].copy()
            logger.info(f"æ‰¾åˆ° {len(valid_df)} ä¸ªå­˜åœ¨çš„å›¾ç‰‡æ–‡ä»¶")
        else:
            valid_df = dataset_df.copy()
            logger.info(f"å¤„ç† {len(valid_df)} ä¸ªå›¾ç‰‡è®°å½• (åŒ…æ‹¬ä¸å­˜åœ¨çš„æ–‡ä»¶)")
        
        if len(valid_df) == 0:
            logger.error("æ²¡æœ‰æ‰¾åˆ°å¯ç”¨çš„å›¾ç‰‡æ–‡ä»¶")
            return
        
        # å»é‡å¤„ç†
        logger.info("å»é™¤é‡å¤æ–‡ä»¶...")
        initial_count = len(valid_df)
        
        # æŒ‰æ–‡ä»¶è·¯å¾„å»é‡ï¼Œä¿ç•™ç¬¬ä¸€æ¡è®°å½•
        valid_df = valid_df.drop_duplicates(subset=['full_image_path'], keep='first')
        
        # ç¡®ä¿IDå”¯ä¸€æ€§
        valid_df = valid_df.drop_duplicates(subset=['id'], keep='first')
        
        deduplicated_count = len(valid_df)
        logger.info(f"å»é‡å®Œæˆ: {initial_count} -> {deduplicated_count}")
        
        if len(valid_df) == 0:
            logger.error("å»é‡åæ²¡æœ‰å¯ç”¨çš„æ•°æ®")
            return
        
        # è·å–å”¯ä¸€çš„å›¾ç‰‡è·¯å¾„
        image_paths = valid_df['full_image_path'].tolist()
        
        # æ‰¹é‡ç¼–ç å›¾ç‰‡ - ä½¿ç”¨å¢å¼ºç‰ˆæ–¹æ³•
        features, valid_paths, error_details = self.clip_encoder.encode_images_batch_from_paths(
            image_paths, batch_size
        )
        
        # è¯¦ç»†åˆ†æé”™è¯¯
        if error_details:
            logger.warning(f"\nâš ï¸ ç¼–ç è¿‡ç¨‹ä¸­å‘ç° {len(error_details)} ä¸ªé”™è¯¯:")
            
            error_stats = {}
            for error in error_details:
                error_type = error['error']
                error_stats[error_type] = error_stats.get(error_type, 0) + 1
            
            for error_type, count in error_stats.items():
                logger.warning(f"   {error_type}: {count} ä¸ª")
            
            # æ˜¾ç¤ºå‰10ä¸ªé”™è¯¯çš„è¯¦ç»†ä¿¡æ¯
            logger.warning(f"\nå‰10ä¸ªé”™è¯¯è¯¦æƒ…:")
            for i, error in enumerate(error_details[:10]):
                logger.warning(f"   {i+1}. {os.path.basename(error['path'])}: {error['message']}")
            
            # ä¿å­˜é”™è¯¯æŠ¥å‘Š
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            error_report_file = f"encoding_errors_{timestamp}.json"
            try:
                with open(error_report_file, 'w', encoding='utf-8') as f:
                    json.dump(error_details, f, ensure_ascii=False, indent=2)
                logger.info(f"é”™è¯¯æŠ¥å‘Šå·²ä¿å­˜: {error_report_file}")
            except:
                pass
        
        if not features:
            logger.error("æ²¡æœ‰æˆåŠŸç¼–ç çš„å›¾ç‰‡")
            return
        
        logger.info(f"ğŸ“Š ç¼–ç ç»Ÿè®¡:")
        logger.info(f"   æ€»å›¾ç‰‡: {len(image_paths)}")
        logger.info(f"   æˆåŠŸç¼–ç : {len(features)}")
        logger.info(f"   ç¼–ç å¤±è´¥: {len(error_details)}")
        logger.info(f"   æˆåŠŸç‡: {len(features)/len(image_paths)*100:.2f}%")
        
        # å‡†å¤‡æ•°æ®æ’å…¥ChromaDB
        embeddings = []
        metadatas = []
        documents = []
        ids = []
        
        used_ids = set()
        used_paths = set()
        
        for i, path in enumerate(valid_paths):
            if path in used_paths:
                continue
            
            matching_rows = valid_df[valid_df['full_image_path'] == path]
            if len(matching_rows) == 0:
                continue
            
            row = matching_rows.iloc[0]
            
            # ç”Ÿæˆå”¯ä¸€ID
            vector_id = f"img_{row['id']}"
            
            # ç¡®ä¿IDå”¯ä¸€
            if vector_id in used_ids:
                counter = 1
                while f"{vector_id}_{counter}" in used_ids:
                    counter += 1
                vector_id = f"{vector_id}_{counter}"
            
            embeddings.append(features[i].tolist())
            
            # æ¸…æ™°çš„å­—æ®µå‘½å
            metadatas.append({
                'id': int(row['id']),
                'image_path': path,
                'original_url': row['image_url'],
                'filename': row['filename'],
                'original_ai_tags': str(row.get('ai_tags', '')),
                'original_tags': str(row.get('tags', '')),
                'combined_tags': str(row['processed_tags']),
                'display_tags': str(row['processed_tags']),
                'created_at': datetime.now().isoformat(),
                'clip_model': self.clip_encoder.model_name
            })
            
            # documentså­—æ®µç”¨äºæœç´¢
            documents.append(f"å›¾ç‰‡: {row['filename']}, æ ‡ç­¾: {row['processed_tags']}")
            ids.append(vector_id)
            
            used_ids.add(vector_id)
            used_paths.add(path)
        
        # æ’å…¥æ•°æ®åº“
        try:
            self.chromadb.add_images(
                embeddings, metadatas, documents, ids, 
                batch_size=chromadb_batch_size
            )
            
            self.is_indexed = True
            logger.info(f"âœ… ç´¢å¼•æ„å»ºå®Œæˆ! æˆåŠŸç´¢å¼• {len(embeddings)} å¼ å›¾ç‰‡")
            
        except Exception as e:
            logger.error(f"æ•°æ®æ’å…¥å¤±è´¥: {e}")
            raise
    
    def search_by_text(self, query_text: str, top_k: int = 9, 
                      search_mode: str = "original") -> List[Dict]:
        """æ ¹æ®æ–‡æœ¬æŸ¥è¯¢ç›¸ä¼¼å›¾ç‰‡"""
        collection_info = self.chromadb.get_collection_info()
        current_count = collection_info.get('count', 0)
        
        if current_count == 0:
            logger.warning("ChromaDBä¸­æ²¡æœ‰æ•°æ®ï¼Œè¯·å…ˆæ„å»ºç´¢å¼•")
            return []
        
        try:
            query_embedding = self.clip_encoder.encode_text(query_text)
            results = self.chromadb.search_similar_images(
                query_embedding.tolist(), 
                top_k=top_k
            )
            
            if not results or not results['ids'] or len(results['ids'][0]) == 0:
                logger.info("æ²¡æœ‰æ‰¾åˆ°ç›¸ä¼¼çš„å›¾ç‰‡")
                return []
            
            formatted_results = []
            for i in range(len(results['ids'][0])):
                metadata = results['metadatas'][0][i]
                distance = results['distances'][0][i]
                similarity = 1 / (1 + distance) if distance > 0 else 1.0
                
                result = {
                    'id': metadata.get('id', ''),
                    'image_path': metadata.get('image_path', ''),
                    'original_url': metadata.get('original_url', ''),
                    'filename': metadata.get('filename', ''),
                    'original_ai_tags': metadata.get('original_ai_tags', ''),
                    'original_tags': metadata.get('original_tags', ''),
                    'combined_tags': metadata.get('combined_tags', ''),
                    'display_tags': metadata.get('display_tags', ''),
                    'similarity': float(similarity),
                    'distance': float(distance),
                    'clip_model': metadata.get('clip_model', ''),
                    'created_at': metadata.get('created_at', ''),
                    'search_type': search_mode
                }
                formatted_results.append(result)
            
            logger.info(f"æ‰¾åˆ° {len(formatted_results)} ä¸ªç›¸ä¼¼ç»“æœ")
            return formatted_results
            
        except Exception as e:
            logger.error(f"æ–‡æœ¬æœç´¢å¤±è´¥: {e}")
            return []
    
    def search_by_image(self, image_path: str, top_k: int = 9) -> List[Dict]:
        """æ ¹æ®å›¾ç‰‡æŸ¥è¯¢ç›¸ä¼¼å›¾ç‰‡"""
        collection_info = self.chromadb.get_collection_info()
        current_count = collection_info.get('count', 0)
        
        if current_count == 0:
            logger.warning("ChromaDBä¸­æ²¡æœ‰æ•°æ®ï¼Œè¯·å…ˆæ„å»ºç´¢å¼•")
            return []
        
        try:
            if not os.path.exists(image_path):
                logger.error(f"å›¾ç‰‡æ–‡ä»¶ä¸å­˜åœ¨: {image_path}")
                return []
            
            query_embedding = self.clip_encoder.encode_image(image_path)
            
            results = self.chromadb.search_similar_images(
                query_embedding.tolist(), 
                top_k=top_k
            )
            
            if not results or not results['ids'] or len(results['ids'][0]) == 0:
                logger.info("æ²¡æœ‰æ‰¾åˆ°ç›¸ä¼¼çš„å›¾ç‰‡")
                return []
            
            formatted_results = []
            for i in range(len(results['ids'][0])):
                metadata = results['metadatas'][0][i]
                distance = results['distances'][0][i]
                
                similarity = 1 / (1 + distance) if distance > 0 else 1.0
                
                result = {
                    'id': metadata.get('id', ''),
                    'image_path': metadata.get('image_path', ''),
                    'original_url': metadata.get('original_url', ''),
                    'filename': metadata.get('filename', ''),
                    'original_ai_tags': metadata.get('original_ai_tags', ''),
                    'original_tags': metadata.get('original_tags', ''),
                    'combined_tags': metadata.get('combined_tags', ''),
                    'display_tags': metadata.get('display_tags', ''),
                    'similarity': float(similarity),
                    'distance': float(distance),
                    'clip_model': metadata.get('clip_model', ''),
                    'created_at': metadata.get('created_at', '')
                }
                formatted_results.append(result)
            
            logger.info(f"æ‰¾åˆ° {len(formatted_results)} ä¸ªç›¸ä¼¼ç»“æœ")
            return formatted_results
            
        except Exception as e:
            logger.error(f"å›¾ç‰‡æœç´¢å¤±è´¥: {e}")
            return []
    
    def get_system_info(self) -> Dict:
        """è·å–ç³»ç»Ÿä¿¡æ¯"""
        try:
            collection_info = self.chromadb.get_collection_info()
            current_count = collection_info.get('count', 0)
            self.is_indexed = current_count > 0
            
            system_info = {
                'clip_model': self.clip_encoder.model_name,
                'feature_dim': self.clip_encoder.feature_dim,
                'device': str(self.clip_encoder.device),
                'is_indexed': self.is_indexed,
                'collection': collection_info
            }
            
            if hasattr(self.db_processor, 'dataset_df') and self.db_processor.dataset_df is not None:
                dataset_info = self.db_processor.get_dataset_info()
                system_info['dataset'] = dataset_info
            
            return system_info
            
        except Exception as e:
            logger.error(f"è·å–ç³»ç»Ÿä¿¡æ¯å¤±è´¥: {e}")
            return {
                'clip_model': getattr(self.clip_encoder, 'model_name', 'unknown'),
                'device': 'unknown',
                'is_indexed': False,
                'collection': {'count': 0},
                'error': str(e)
            }
    
    def close_connections(self):
        """å…³é—­æ‰€æœ‰è¿æ¥"""
        self.db_processor.close_connection()
        logger.info("æ‰€æœ‰æ•°æ®åº“è¿æ¥å·²å…³é—­")

class EnhancedDatabaseImageRetrievalSystem(DatabaseImageRetrievalSystem):
    """å¢å¼ºçš„å›¾ç‰‡æ£€ç´¢ç³»ç»Ÿï¼ˆé›†æˆOpenRouterï¼‰"""
    
    def __init__(self, 
                 clip_model: str = "ViT-B/32",
                 chromadb_host: str = "localhost", 
                 chromadb_port: int = 6600,
                 collection_name: str = "local_db_image_collection",
                 openrouter_api_key: str = None,
                 openrouter_model: str = "anthropic/claude-3-haiku"):
        """
        åˆå§‹åŒ–å¢å¼ºæ£€ç´¢ç³»ç»Ÿ
        Args:
            openrouter_api_key: OpenRouter APIå¯†é’¥
            openrouter_model: ä½¿ç”¨çš„æ¨¡å‹
        """
        # è°ƒç”¨çˆ¶ç±»åˆå§‹åŒ–
        super().__init__(clip_model, chromadb_host, chromadb_port, collection_name)
        
        # åˆå§‹åŒ–OpenRouterå¤„ç†å™¨
        self.openrouter = None
        if openrouter_api_key:
            try:
                self.openrouter = OpenRouterProcessor(openrouter_api_key, openrouter_model)
                logger.info("âœ… OpenRouterå¤„ç†å™¨åˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                logger.warning(f"âš ï¸ OpenRouteråˆå§‹åŒ–å¤±è´¥ï¼Œå°†ä½¿ç”¨å¤‡ç”¨æ–¹æ³•: {e}")
        else:
            logger.info("ğŸ’¡ æœªæä¾›OpenRouter APIå¯†é’¥ï¼Œå°†ä½¿ç”¨å¤‡ç”¨åˆ†ææ–¹æ³•")
        
        # æ„å»ºå¯ç”¨æ ‡ç­¾åˆ—è¡¨
        self.available_tags = self._build_available_tags()
        logger.info(f"ğŸ“‹ æ„å»ºäº† {len(self.available_tags)} ä¸ªå¯ç”¨æ ‡ç­¾")
    
    def _build_available_tags(self) -> List[str]:
        """æ„å»ºæ‰€æœ‰å¯ç”¨æ ‡ç­¾çš„æ‰å¹³åˆ—è¡¨"""
        all_tags = []
        for category, tags in self.tag_keywords.items():
            all_tags.extend(tags)
        return list(set(all_tags))
    
    def search_by_text_intelligent(self, user_query: str, top_k: int = 9,
                             tag_weight: float = 0.6, visual_weight: float = 0.4) -> List[Dict]:
        """
        æ™ºèƒ½æ–‡æœ¬æœç´¢ï¼ˆé›†æˆLLMåˆ†æï¼‰
        """
        collection_info = self.chromadb.get_collection_info()
        current_count = collection_info.get('count', 0)
        
        if current_count == 0:
            logger.warning("ChromaDBä¸­æ²¡æœ‰æ•°æ®ï¼Œè¯·å…ˆæ„å»ºç´¢å¼•")
            return []
        
        logger.info(f"å¼€å§‹æ™ºèƒ½æœç´¢: '{user_query}'")
        
        try:
            # LLMåˆ†æç”¨æˆ·æŸ¥è¯¢
            if self.openrouter:
                logger.info("ğŸ¤– ä½¿ç”¨LLMåˆ†æç”¨æˆ·æŸ¥è¯¢...")
                query_analysis = self.openrouter.analyze_query(user_query, self.available_tags)
            else:
                logger.info("ğŸ”„ ä½¿ç”¨å¤‡ç”¨æ–¹æ³•åˆ†ææŸ¥è¯¢...")
                query_analysis = self._fallback_query_analysis(user_query)
            
            # æ˜¾ç¤ºåˆ†æç»“æœ
            self._log_analysis_result(query_analysis)
            
            # æ„å»ºä¼˜åŒ–çš„CLIPæŸ¥è¯¢
            optimized_clip_query = self._build_optimized_clip_query(query_analysis, user_query)
            
            # CLIPè§†è§‰æœç´¢
            visual_results = self._get_visual_results_optimized(optimized_clip_query, top_k)
            
            # æ·»åŠ åˆ†æä¿¡æ¯åˆ°ç»“æœä¸­
            for result in visual_results:
                result['query_analysis'] = query_analysis
                result['optimized_query'] = optimized_clip_query
                result['search_type'] = 'intelligent'
            
            logger.info(f"æ™ºèƒ½æœç´¢å®Œæˆï¼Œè¿”å› {len(visual_results)} ä¸ªç»“æœ")
            return visual_results
            
        except Exception as e:
            logger.error(f"æ™ºèƒ½æœç´¢å¤±è´¥: {e}")
            # å›é€€åˆ°åŸºç¡€æœç´¢
            return self.search_by_text(user_query, top_k)
    
    def _log_analysis_result(self, analysis: Dict):
        """è®°å½•åˆ†æç»“æœ"""
        logger.info("ğŸ§  LLMåˆ†æç»“æœ:")
        logger.info(f"   æ€»ç»“: {analysis.get('summary', '')}")
        logger.info(f"   åœºæ™¯ç±»å‹: {analysis.get('scene_type', '')}")
        logger.info(f"   é£æ ¼åå¥½: {analysis.get('style_preference', '')}")
        logger.info(f"   æ ¸å¿ƒæ¦‚å¿µ: {analysis.get('key_concepts', [])}")
        logger.info(f"   è§†è§‰å…³é”®è¯: {analysis.get('visual_keywords', [])}")
        logger.info(f"   æœç´¢ç­–ç•¥: {analysis.get('search_strategy', 'balanced')}")
        
        matched_tags = analysis.get('matched_tags', {})
        if matched_tags:
            logger.info("   åŒ¹é…çš„æ ‡ç­¾:")
            for category, tags in matched_tags.items():
                if tags:
                    logger.info(f"     {category}: {tags}")
    
    def _build_optimized_clip_query(self, analysis: Dict, original_query: str) -> str:
        """æ„å»ºä¼˜åŒ–çš„CLIPæŸ¥è¯¢"""
        try:
            # è·å–è‹±æ–‡è§†è§‰å…³é”®è¯
            visual_keywords = analysis.get('visual_keywords', [])
            
            # åœºæ™¯å’Œé£æ ¼ä¿¡æ¯
            scene_type = analysis.get('scene_type', '')
            style_preference = analysis.get('style_preference', '')
            
            # æ„å»ºä¼˜åŒ–æŸ¥è¯¢
            query_parts = []
            
            # æ·»åŠ åŸå§‹æŸ¥è¯¢
            query_parts.append(original_query)
            
            # æ·»åŠ è‹±æ–‡è§†è§‰å…³é”®è¯
            if visual_keywords:
                query_parts.extend(visual_keywords)
            
            # æ·»åŠ åœºæ™¯æè¿°
            if scene_type:
                scene_mapping = {
                    'å®¶åº­': 'family scene',
                    'å•†åŠ¡': 'business scene',
                    'ä¼‘é—²': 'leisure scene',
                    'åŸå¸‚': 'urban scene',
                    'è‡ªç„¶': 'natural scene'
                }
                english_scene = scene_mapping.get(scene_type, scene_type)
                query_parts.append(english_scene)
            
            optimized_query = ' '.join(query_parts)
            
            logger.info(f"ä¼˜åŒ–çš„CLIPæŸ¥è¯¢: '{optimized_query}'")
            return optimized_query
            
        except Exception as e:
            logger.error(f"æ„å»ºä¼˜åŒ–æŸ¥è¯¢å¤±è´¥: {e}")
            return original_query
    
    def _get_visual_results_optimized(self, optimized_query: str, top_k: int) -> List[Dict]:
        """ä½¿ç”¨ä¼˜åŒ–æŸ¥è¯¢è·å–è§†è§‰ç»“æœ"""
        try:
            query_embedding = self.clip_encoder.encode_text(optimized_query)
            if query_embedding is None:
                return []
            
            results = self.chromadb.search_similar_images(
                query_embedding.tolist(), 
                top_k=top_k
            )
            
            if not results or not results['ids'] or len(results['ids'][0]) == 0:
                return []
            
            visual_results = []
            for i in range(len(results['ids'][0])):
                metadata = results['metadatas'][0][i]
                distance = results['distances'][0][i]
                similarity = 1 / (1 + distance) if distance > 0 else 1.0
                
                # å®‰å…¨åœ°è·å–æ‰€æœ‰å­—æ®µ
                result = {
                    'id': metadata.get('id', ''),
                    'image_path': metadata.get('image_path', ''),
                    'original_url': metadata.get('original_url', ''),
                    'filename': metadata.get('filename', ''),
                    'original_ai_tags': metadata.get('original_ai_tags', ''),
                    'original_tags': metadata.get('original_tags', ''),
                    'combined_tags': metadata.get('combined_tags', ''),
                    'display_tags': metadata.get('display_tags', ''),
                    'visual_score': float(similarity),
                    'similarity': float(similarity),
                    'distance': float(distance),
                    'clip_model': metadata.get('clip_model', ''),
                    'created_at': metadata.get('created_at', '')
                }
                
                visual_results.append(result)
            
            logger.info(f"ä¼˜åŒ–è§†è§‰æœç´¢è·å¾— {len(visual_results)} ä¸ªç»“æœ")
            return visual_results
            
        except Exception as e:
            logger.error(f"ä¼˜åŒ–è§†è§‰æœç´¢å¤±è´¥: {e}")
            return []
    
    def _fallback_query_analysis(self, user_query: str) -> Dict:
        """å¤‡ç”¨æŸ¥è¯¢åˆ†æï¼ˆä¸ä½¿ç”¨LLMï¼‰"""
        logger.info("ä½¿ç”¨å¤‡ç”¨æŸ¥è¯¢åˆ†ææ–¹æ³•")
        
        query_lower = user_query.lower()
        
        # ç®€å•çš„åœºæ™¯è¯†åˆ«
        scene_keywords = {
            "å®¶åº­": ["å®¶åº­", "äº²å­", "å®¶äºº", "æ¸©é¦¨", "å±…å®¶"],
            "å•†åŠ¡": ["å•†åŠ¡", "åŠå…¬", "å·¥ä½œ", "æ­£å¼", "ä¸“ä¸š"],
            "ä¼‘é—²": ["ä¼‘é—²", "æ”¾æ¾", "åº¦å‡", "å¨±ä¹", "æ—…è¡Œ"],
            "åŸå¸‚": ["åŸå¸‚", "éƒ½å¸‚", "è¡—é“", "å¸‚åŒº", "å»ºç­‘"],
            "è‡ªç„¶": ["è‡ªç„¶", "é£æ™¯", "æˆ·å¤–", "å±±", "æµ·", "ç”°é‡"]
        }
        
        detected_scene = "é€šç”¨"
        for scene, keywords in scene_keywords.items():
            if any(keyword in query_lower for keyword in keywords):
                detected_scene = scene
                break
        
        # åŒ¹é…å¯ç”¨æ ‡ç­¾
        matched_tags = []
        for tag in self.available_tags:
            if tag.lower() in query_lower:
                matched_tags.append(tag)
        
        return {
            "summary": f"æŸ¥è¯¢: {user_query}",
            "key_concepts": user_query.split()[:5],
            "visual_keywords": user_query.split()[:3],
            "matched_tags": {"é€šç”¨": matched_tags[:10]},
            "scene_type": detected_scene,
            "style_preference": "æœªæŒ‡å®š",
            "search_strategy": "balanced"
        }
    
    def check_index_status(self) -> Dict:
        """æ£€æŸ¥ç´¢å¼•çŠ¶æ€å’Œæ•°æ®æ›´æ–°"""
        try:
            # æ£€æŸ¥ChromaDBä¸­çš„æ•°æ®
            collection_info = self.chromadb.get_collection_info()
            indexed_count = collection_info.get('count', 0)
            
            # æ£€æŸ¥æ•°æ®åº“æ›´æ–°
            update_info = self.db_processor.check_data_updates()
            
            # åˆ¤æ–­æ˜¯å¦éœ€è¦é‡å»ºç´¢å¼•
            need_rebuild = False
            rebuild_reason = []
            
            # å¦‚æœæ²¡æœ‰ç´¢å¼•
            if indexed_count == 0:
                need_rebuild = True
                rebuild_reason.append("æ²¡æœ‰ç°æœ‰ç´¢å¼•")
            
            # å¦‚æœæ•°æ®åº“æœ‰æ›´æ–°
            elif update_info['has_updates']:
                need_rebuild = True
                rebuild_reason.append(f"æ•°æ®åº“{update_info['change_type']}: {update_info['change_count']:,} æ¡è®°å½•")
            
            # å¦‚æœç´¢å¼•æ•°é‡ä¸æ•°æ®åº“ä¸åŒ¹é…
            elif update_info['current_count'] > 0:
                expected_count = update_info['current_count']
                if abs(indexed_count - expected_count) > 100:  # å…è®¸å°è¯¯å·®
                    need_rebuild = True
                    rebuild_reason.append(f"ç´¢å¼•ä¸åŒ¹é…: ChromaDB({indexed_count:,}) vs æ•°æ®åº“({expected_count:,})")
            
            return {
                'indexed_count': indexed_count,
                'database_count': update_info['current_count'],
                'need_rebuild': need_rebuild,
                'rebuild_reason': rebuild_reason,
                'update_info': update_info,
                'collection_info': collection_info
            }
            
        except Exception as e:
            logger.error(f"æ£€æŸ¥ç´¢å¼•çŠ¶æ€å¤±è´¥: {e}")
            return {
                'indexed_count': 0,
                'database_count': 0,
                'need_rebuild': True,
                'rebuild_reason': [f"æ£€æŸ¥å¤±è´¥: {e}"],
                'update_info': {'has_updates': False, 'message': 'Unknown'},
                'error': str(e)
            }
            
    def smart_index_management(self, force_rebuild: bool = False, 
                              limit: int = 183247, batch_size: int = 32,
                              chromadb_batch_size: int = 4000) -> bool:
        """æ™ºèƒ½ç´¢å¼•ç®¡ç†"""
        try:
            print("\nğŸ” æ£€æŸ¥ç´¢å¼•çŠ¶æ€...")
            
            status = self.check_index_status()
            
            print(f"ğŸ“Š å½“å‰çŠ¶æ€:")
            print(f"   ChromaDBç´¢å¼•: {status['indexed_count']:,} æ¡")
            print(f"   æ•°æ®åº“è®°å½•: {status['database_count']:,} æ¡")
            
            # æ˜¾ç¤ºæ›´æ–°ä¿¡æ¯
            update_info = status['update_info']
            print(f"   {update_info['message']}")
            
            if update_info.get('check_time'):
                check_time = datetime.fromisoformat(update_info['check_time'])
                print(f"   æ£€æŸ¥æ—¶é—´: {check_time.strftime('%Y-%m-%d %H:%M:%S')}")
            
            # å†³å®šæ˜¯å¦é‡å»º
            if force_rebuild:
                print("ğŸ”„ å¼ºåˆ¶é‡å»ºç´¢å¼•...")
                should_rebuild = True
            elif not status['need_rebuild']:
                print("âœ… ç´¢å¼•çŠ¶æ€è‰¯å¥½ï¼Œæ— éœ€é‡å»º")
                return True
            else:
                print(f"\nâš ï¸ æ£€æµ‹åˆ°éœ€è¦é‡å»ºç´¢å¼•:")
                for reason in status['rebuild_reason']:
                    print(f"   â€¢ {reason}")
                
                print(f"\nğŸ“ˆ è¯¦ç»†å˜åŒ–:")
                if update_info['change_type'] == 'increased':
                    print(f"   æ–°å¢ {update_info['change_count']:,} æ¡è®°å½•")
                elif update_info['change_type'] == 'decreased':
                    print(f"   å‡å°‘ {update_info['change_count']:,} æ¡è®°å½•")
                elif update_info['change_type'] == 'initial':
                    print(f"   åˆå§‹åŒ–ï¼Œéœ€è¦ç´¢å¼• {update_info['current_count']:,} æ¡è®°å½•")
                
                # è¯¢é—®ç”¨æˆ·
                response = input("\nğŸ¤” æ˜¯å¦é‡å»ºç´¢å¼•? (y/n): ").lower().strip()
                should_rebuild = response in ['y', 'yes', 'æ˜¯']
                
                if not should_rebuild:
                    print("â­ï¸ è·³è¿‡ç´¢å¼•é‡å»ºï¼Œä½¿ç”¨ç°æœ‰ç´¢å¼•")
                    return True
            
            # æ‰§è¡Œé‡å»º
            if should_rebuild:
                print(f"\nğŸš€ å¼€å§‹é‡å»ºç´¢å¼• (é™åˆ¶: {limit:,} æ¡è®°å½•)...")
                
                # é‡ç½®é›†åˆ
                if status['indexed_count'] > 0:
                    print("ğŸ—‘ï¸ æ¸…ç†ç°æœ‰ç´¢å¼•...")
                    self.chromadb.reset_collection()
                
                # åŠ è½½æ•°æ®ï¼ˆä¼šè‡ªåŠ¨ä¿å­˜çŠ¶æ€ï¼‰
                print("ğŸ“Š åŠ è½½æ•°æ®åº“æ•°æ®...")
                self.db_processor.load_data(limit=limit, save_status=True)
                
                # æ„å»ºç´¢å¼•
                print("ğŸ”¨ æ„å»ºæ–°ç´¢å¼•...")
                self.build_index(
                    batch_size=batch_size, 
                    force_rebuild=True, 
                    limit=limit,
                    chromadb_batch_size=chromadb_batch_size
                )
                
                # éªŒè¯ç»“æœ
                final_status = self.check_index_status()
                print(f"\nâœ… ç´¢å¼•é‡å»ºå®Œæˆ:")
                print(f"   æœ€ç»ˆç´¢å¼•é‡: {final_status['indexed_count']:,} æ¡")
                print(f"   æ•°æ®åº“æ€»é‡: {final_status['database_count']:,} æ¡")
                
                return True
            
            return False
            
        except Exception as e:
            logger.error(f"æ™ºèƒ½ç´¢å¼•ç®¡ç†å¤±è´¥: {e}")
            print(f"âŒ ç´¢å¼•ç®¡ç†å¤±è´¥: {e}")
            return False

class VisualizationTool:
    """å¯è§†åŒ–å·¥å…·"""
    
    def __init__(self):
        """åˆå§‹åŒ–å¯è§†åŒ–å·¥å…·"""
        self.figure_count = 0
    
    def load_image_from_path(self, image_path: str) -> Optional[Image.Image]:
        """ä»æœ¬åœ°è·¯å¾„åŠ è½½å›¾ç‰‡"""
        try:
            if not os.path.exists(image_path):
                logger.warning(f"å›¾ç‰‡æ–‡ä»¶ä¸å­˜åœ¨: {image_path}")
                return None
            
            image = Image.open(image_path).convert('RGB')
            return image
        except Exception as e:
            logger.warning(f"å›¾ç‰‡åŠ è½½å¤±è´¥ {image_path}: {e}")
            return None
    
    def show_search_results(self, query: str, results: List[Dict], 
                          max_display: int = 9, figsize: Tuple[int, int] = (15, 12)):
        """æ˜¾ç¤ºæœç´¢ç»“æœ"""
        if not results:
            print("æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ç»“æœ")
            return
        
        display_count = min(len(results), max_display)
        cols = 3
        rows = (display_count + cols - 1) // cols
        
        fig, axes = plt.subplots(rows, cols, figsize=figsize)
        if rows == 1:
            axes = axes.reshape(1, -1)
        elif cols == 1:
            axes = axes.reshape(-1, 1)
        
        query_type = results[0].get('query_type', 'text')
        
        if query_type == 'image':
            fig.suptitle(f'ä»¥å›¾æœå›¾ç»“æœ: {query}', fontsize=16, fontweight='bold')
        else:
            fig.suptitle(f'ä»¥æ–‡æœå›¾ç»“æœ: "{query[:50]}"', fontsize=16, fontweight='bold')
        
        for i in range(display_count):
            row, col = i // cols, i % cols
            ax = axes[row, col] if rows > 1 else axes[col]
            
            result = results[i]
            
            # åŠ è½½å¹¶æ˜¾ç¤ºå›¾ç‰‡
            image = self.load_image_from_path(result['image_path'])
            
            if image is not None:
                ax.imshow(image)
                
                # è®¾ç½®æ ‡é¢˜
                title = f"ç›¸ä¼¼åº¦: {result['similarity']:.3f}\nID: {result['id']}"
                
                # æ˜¾ç¤ºåˆå¹¶åçš„æ ‡ç­¾
                combined_tags = result.get('combined_tags') or result.get('display_tags', '')
                if combined_tags:
                    tags_display = combined_tags[:30]
                    if len(combined_tags) > 30:
                        tags_display += "..."
                else:
                    tags_display = "æ— æ ‡ç­¾"
                
                title += f"\n{tags_display}"
                
                ax.set_title(title, fontsize=9)
                ax.axis('off')
            else:
                ax.text(0.5, 0.5, f"å›¾ç‰‡åŠ è½½å¤±è´¥\nID: {result['id']}\n{result['filename']}", 
                       ha='center', va='center', transform=ax.transAxes,
                       fontsize=8, bbox=dict(boxstyle="round,pad=0.3", facecolor="red", alpha=0.3))
                ax.set_title(f"é”™è¯¯: {result['filename']}", fontsize=10)
                ax.axis('off')
        
        # éšè—å¤šä½™çš„å­å›¾
        for i in range(display_count, rows * cols):
            row, col = i // cols, i % cols
            ax = axes[row, col] if rows > 1 else axes[col]
            ax.axis('off')
        
        plt.tight_layout()
        self.figure_count += 1
        plt.show()

# ç»Ÿä¸€çš„ç»“æœæ˜¾ç¤ºå‡½æ•°
def display_search_results(results, query_info=""):
    """ç»Ÿä¸€çš„æœç´¢ç»“æœæ˜¾ç¤ºå‡½æ•° - ä¿®å¤ç‰ˆæœ¬"""
    if not results:
        print("âŒ æœªæ‰¾åˆ°ç›¸å…³ç»“æœ")
        return
    
    print(f"\nâœ… æ‰¾åˆ° {len(results)} ä¸ªç»“æœ")
    
    # æ˜¾ç¤ºæŸ¥è¯¢åˆ†æï¼ˆå¦‚æœæ˜¯æ™ºèƒ½æœç´¢ï¼‰
    if results and results[0].get('query_analysis'):
        analysis = results[0].get('query_analysis', {})
        print(f"\nğŸ§  æœç´¢åˆ†æ:")
        print(f"   ç†è§£: {analysis.get('summary', '')}")
        print(f"   åœºæ™¯: {analysis.get('scene_type', '')}")
        print(f"   é£æ ¼: {analysis.get('style_preference', '')}")
        print(f"   ç­–ç•¥: {analysis.get('search_strategy', '')}")
        
        optimized_query = results[0].get('optimized_query', '')
        if optimized_query and optimized_query != query_info:
            print(f"   ä¼˜åŒ–æŸ¥è¯¢: {optimized_query}")
    
    # æ˜¾ç¤ºç»“æœåˆ—è¡¨
    print(f"\nğŸ“‹ æœç´¢ç»“æœ:")
    for i, result in enumerate(results, 1):
        similarity = result.get('similarity', 0)
        print(f"  {i:2d}. ç›¸ä¼¼åº¦: {similarity:.4f}")
        print(f"      ID: {result.get('id', 'N/A')}")
        print(f"      æ–‡ä»¶å: {result.get('filename', 'N/A')}")
        print(f"      å®Œæ•´è·¯å¾„: {result.get('image_path', 'N/A')}")
        
        # å®‰å…¨åœ°è·å–åŸå§‹URL
        original_url = result.get('original_url', 'N/A')
        if original_url and original_url != 'N/A':
            print(f"      åŸå§‹URL: {original_url}")
        else:
            print(f"      åŸå§‹URL: æœªæ‰¾åˆ°")
        
        # ä¿®å¤çš„æ ‡ç­¾æ˜¾ç¤ºé€»è¾‘
        display_tags = (
            result.get('combined_tags') or
            result.get('display_tags') or
            result.get('original_ai_tags') or
            result.get('original_tags', '')
        )
        
        if display_tags and str(display_tags).strip() and str(display_tags).strip() not in ['nan', 'æ— æ ‡ç­¾ä¿¡æ¯', 'å›¾ç‰‡ä¸ºç©º']:
            display_tags_str = str(display_tags)
            
            if display_tags_str.strip() == 'å›¾ç‰‡ä¸ºç©º':
                print(f"      æ ‡ç­¾: æ— æœ‰æ•ˆæ ‡ç­¾ä¿¡æ¯")
            else:
                if len(display_tags_str) > 150:
                    print(f"      æ ‡ç­¾: {display_tags_str[:150]}...")
                else:
                    print(f"      æ ‡ç­¾: {display_tags_str}")
        else:
            print(f"      æ ‡ç­¾: æ— æ ‡ç­¾ä¿¡æ¯")
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        image_path = result.get('image_path', '')
        if image_path:
            file_exists = os.path.exists(image_path)
            print(f"      æ–‡ä»¶çŠ¶æ€: {'âœ… å­˜åœ¨' if file_exists else 'âŒ ä¸å­˜åœ¨'}")
        else:
            print(f"      æ–‡ä»¶çŠ¶æ€: âŒ æ— è·¯å¾„ä¿¡æ¯")
        print()

def main():
    """ä¸»å‡½æ•° - æ”¯æŒLLMå¢å¼ºçš„æ™ºèƒ½æœç´¢"""
    print("=" * 60)
    print("ğŸ¨ CLIP + ChromaDB + MySQL + LLM æ™ºèƒ½å›¾ç‰‡æ£€ç´¢ç³»ç»Ÿ v2.1")  
    print("ğŸ¤– é›†æˆOpenRouterå¤§è¯­è¨€æ¨¡å‹è¿›è¡Œè‡ªç„¶è¯­è¨€ç†è§£")
    print("ğŸ  å¤„ç†æœ¬åœ°ç»å¯¹è·¯å¾„å›¾ç‰‡")
    print("ğŸ“Š æ™ºèƒ½ç´¢å¼•ç®¡ç†å’Œå¢é‡æ›´æ–°æ£€æµ‹")
    print("=" * 60)
    
    # OpenRouteré…ç½®
    print("\nğŸ”§ é…ç½®OpenRouter (å¯é€‰):")
    print("å¦‚æœæ‚¨æœ‰OpenRouter APIå¯†é’¥ï¼Œå¯ä»¥å¯ç”¨LLMå¢å¼ºæœç´¢")
    print("æ²¡æœ‰å¯†é’¥ä¹Ÿå¯ä»¥ä½¿ç”¨åŸºç¡€æœç´¢åŠŸèƒ½")
    
    use_openrouter = input("æ˜¯å¦é…ç½®OpenRouter? (y/n): ").lower() == 'y'
    openrouter_api_key = None
    openrouter_model = "anthropic/claude-3-haiku"
    
    if use_openrouter:
        openrouter_api_key = input("è¯·è¾“å…¥OpenRouter APIå¯†é’¥: ").strip()
        if not openrouter_api_key:
            print("âš ï¸ æœªæä¾›APIå¯†é’¥ï¼Œå°†ä½¿ç”¨åŸºç¡€æœç´¢åŠŸèƒ½")
        else:
            print("\nğŸ¤– é€‰æ‹©LLMæ¨¡å‹:")
            models = [
                "anthropic/claude-3-haiku",
                "anthropic/claude-3-5-sonnet", 
                "openai/gpt-4o-mini",
                "openai/gpt-3.5-turbo",
                "meta-llama/llama-3.1-8b-instruct"
            ]
            for i, model in enumerate(models, 1):
                print(f"{i}. {model}")
            
            model_choice = input("è¯·é€‰æ‹©æ¨¡å‹ (é»˜è®¤: 1): ").strip()
            try:
                model_index = int(model_choice) - 1 if model_choice else 0
                openrouter_model = models[model_index]
            except:
                openrouter_model = "anthropic/claude-3-haiku"
            
            print(f"é€‰æ‹©çš„æ¨¡å‹: {openrouter_model}")
    
    # CLIPæ¨¡å‹é€‰æ‹©
    print("\nğŸ¤– é€‰æ‹©CLIPæ¨¡å‹:")
    clip_models = ["ViT-B/32", "ViT-B/16", "ViT-L/14", "RN50"]
    for i, model in enumerate(clip_models, 1):
        print(f"{i}. {model}")
    
    model_choice = input("è¯·é€‰æ‹©CLIPæ¨¡å‹ (é»˜è®¤: 1): ").strip()
    try:
        model_index = int(model_choice) - 1 if model_choice else 0
        clip_model = clip_models[model_index]
    except:
        clip_model = "ViT-B/32"
    
    try:
        # åˆå§‹åŒ–å¢å¼ºæ£€ç´¢ç³»ç»Ÿ
        print("\nğŸš€ åˆå§‹åŒ–æ™ºèƒ½æ£€ç´¢ç³»ç»Ÿ...")
        
        retrieval_system = EnhancedDatabaseImageRetrievalSystem(
            clip_model=clip_model,
            chromadb_port=6600,
            openrouter_api_key=openrouter_api_key,
            openrouter_model=openrouter_model
        )
        
        # æ™ºèƒ½ç´¢å¼•ç®¡ç†
        print("\nğŸ“Š æ™ºèƒ½ç´¢å¼•ç®¡ç†...")
        retrieval_system.smart_index_management(
            force_rebuild=False,  # ä¸å¼ºåˆ¶é‡å»ºï¼Œè®©ç³»ç»Ÿæ™ºèƒ½åˆ¤æ–­
            limit=183247
        )
        
        # æ˜¾ç¤ºæœ€ç»ˆç³»ç»Ÿä¿¡æ¯
        system_info = retrieval_system.get_system_info()
        print("\nğŸ“Š ç³»ç»Ÿå°±ç»ª:")
        print(f"   LLMé›†æˆ: {'âœ… å·²å¯ç”¨' if retrieval_system.openrouter else 'âŒ æœªå¯ç”¨'}")
        if retrieval_system.openrouter:
            print(f"   LLMæ¨¡å‹: {openrouter_model}")
        print(f"   CLIPæ¨¡å‹: {clip_model}")
        print(f"   å·²ç´¢å¼•: {system_info['collection']['count']:,} å¼ å›¾ç‰‡")
        print(f"   å¯ç”¨æ ‡ç­¾: {len(retrieval_system.available_tags)} ä¸ª")
        
        # åˆå§‹åŒ–å¯è§†åŒ–å·¥å…·
        viz_tool = VisualizationTool()
        
        # äº¤äº’å¼æœç´¢å¾ªç¯
        while True:
            print("\n" + "="*60)
            print("ğŸ” æ™ºèƒ½å›¾ç‰‡æ£€ç´¢ç³»ç»Ÿ v2.1")
            
            if retrieval_system.openrouter:
                print("1. æ™ºèƒ½æœç´¢ (LLM + CLIP + æ ‡ç­¾)")
                print("2. åŸºç¡€æœç´¢ (CLIP)")
                print("3. ä»¥å›¾æœå›¾")
            else:
                print("1. åŸºç¡€æœç´¢ (CLIP)")
                print("2. ä»¥å›¾æœå›¾")
                
            print("5. æ‰‹åŠ¨æ£€æŸ¥æ›´æ–°")
            print("6. æ£€æŸ¥æ•°æ®å®Œæ•´æ€§")
            print("7. å¼ºåˆ¶é‡å»ºç´¢å¼•")
            print("8. æŸ¥çœ‹ç³»ç»ŸçŠ¶æ€")
            print("9. é€€å‡º")
            
            choice = input("è¯·é€‰æ‹©åŠŸèƒ½: ").strip()
            
            # æ™ºèƒ½æœç´¢æˆ–åŸºç¡€æœç´¢
            if choice == '1':
                if retrieval_system.openrouter:
                    search_mode = 'intelligent'
                    print("\nğŸ§  æ™ºèƒ½æœç´¢ (LLMç†è§£ + å¤šç»´åº¦åŒ¹é…)")
                    print("ğŸ’¡ æ™ºèƒ½æœç´¢å»ºè®®:")
                    print("   â€¢ ç”¨è‡ªç„¶è¯­è¨€æè¿°ï¼šå¦‚'æ¸©é¦¨çš„å®¶åº­å‡ºæ¸¸åœºæ™¯'")
                    print("   â€¢ æè¿°æƒ…æ„Ÿå’Œæ°›å›´ï¼šå¦‚'å•†åŠ¡æ­£å¼æ„Ÿ'ã€'å¹´è½»æ´»åŠ›'")
                    print("   â€¢ åŒ…å«åœºæ™¯ä¿¡æ¯ï¼šå¦‚'åŸå¸‚è¡—é“'ã€'è‡ªç„¶é£å…‰'")
                    print("   â€¢ æåŠé£æ ¼åå¥½ï¼šå¦‚'ç°ä»£ç®€çº¦'ã€'ç»å…¸ä¼˜é›…'")
                else:
                    search_mode = 'basic'
                    print("\nğŸ” åŸºç¡€æœç´¢ (CLIPè§†è§‰ç‰¹å¾)")
                
                user_query = input("ğŸ“ è¯·è¾“å…¥æœç´¢æè¿°: ").strip()
                if not user_query:
                    continue
                
                top_k = input("è¿”å›ç»“æœæ•°é‡ (é»˜è®¤: 9): ").strip()
                try:
                    top_k = int(top_k) if top_k else 9
                except:
                    top_k = 9
                
                print(f"\nğŸ” å¼€å§‹æœç´¢: '{user_query}'")
                
                try:
                    if search_mode == 'intelligent':
                        results = retrieval_system.search_by_text_intelligent(user_query, top_k)
                    else:
                        results = retrieval_system.search_by_text(user_query, top_k)
                    
                    # æ˜¾ç¤ºç»“æœ
                    display_search_results(results, user_query)
                    
                    # å¯è§†åŒ–é€‰é¡¹
                    if results:
                        show_viz = input("ğŸ“Š æ˜¾ç¤ºå›¾ç‰‡ç»“æœ? (y/n): ").lower() == 'y'
                        if show_viz:
                            viz_tool.show_search_results(user_query, results, max_display=top_k)
                            
                except Exception as e:
                    print(f"âŒ æœç´¢å¤±è´¥: {e}")
                    logger.exception("æœç´¢é”™è¯¯")
            
            # åŸºç¡€æœç´¢æˆ–ä»¥å›¾æœå›¾
            elif choice == '2':
                if retrieval_system.openrouter:
                    print("\nğŸ” åŸºç¡€æœç´¢ (CLIPè§†è§‰ç‰¹å¾)")
                    user_query = input("ğŸ“ è¯·è¾“å…¥æœç´¢æè¿°: ").strip()
                    if not user_query:
                        continue
                    
                    top_k = input("è¿”å›ç»“æœæ•°é‡ (é»˜è®¤: 9): ").strip()
                    try:
                        top_k = int(top_k) if top_k else 9
                    except:
                        top_k = 9
                    
                    print(f"\nğŸ” å¼€å§‹åŸºç¡€æœç´¢: '{user_query}'")
                    results = retrieval_system.search_by_text(user_query, top_k)
                    
                    # æ˜¾ç¤ºç»“æœ
                    display_search_results(results, user_query)
                    
                    # å¯è§†åŒ–é€‰é¡¹
                    if results:
                        show_viz = input("ğŸ“Š æ˜¾ç¤ºå›¾ç‰‡ç»“æœ? (y/n): ").lower() == 'y'
                        if show_viz:
                            viz_tool.show_search_results(user_query, results, max_display=top_k)
                else:
                    # æ²¡æœ‰OpenRouteræ—¶ï¼Œé€‰é¡¹2æ˜¯ä»¥å›¾æœå›¾
                    print("\nğŸ–¼ï¸ ä»¥å›¾æœå›¾")
                    query_image_path = input("è¯·è¾“å…¥å›¾ç‰‡è·¯å¾„: ").strip()
                    if query_image_path and os.path.exists(query_image_path):
                        top_k = input("è¿”å›ç»“æœæ•°é‡ (é»˜è®¤: 9): ").strip()
                        top_k = int(top_k) if top_k.isdigit() else 9
                        
                        print(f"\nğŸ” å¼€å§‹ä»¥å›¾æœå›¾: '{os.path.basename(query_image_path)}'")
                        results = retrieval_system.search_by_image(query_image_path, top_k)
                        
                        if results:
                            for result in results:
                                result['query_type'] = 'image'
                            
                            display_search_results(results, f"æŸ¥è¯¢å›¾ç‰‡: {os.path.basename(query_image_path)}")
                            
                            show_viz = input("ğŸ“Š æ˜¾ç¤ºå›¾ç‰‡ç»“æœ? (y/n): ").lower() == 'y'
                            if show_viz:
                                viz_tool.show_search_results(f"æŸ¥è¯¢å›¾ç‰‡: {os.path.basename(query_image_path)}", results, max_display=top_k)
                        else:
                            print("âŒ æœªæ‰¾åˆ°ç›¸ä¼¼å›¾ç‰‡")
                    else:
                        print("âŒ å›¾ç‰‡æ–‡ä»¶ä¸å­˜åœ¨")
            
            # ä»¥å›¾æœå›¾ï¼ˆå½“æœ‰OpenRouteræ—¶ï¼‰
            elif choice == '3' and retrieval_system.openrouter:
                print("\nğŸ–¼ï¸ ä»¥å›¾æœå›¾")
                query_image_path = input("è¯·è¾“å…¥å›¾ç‰‡è·¯å¾„: ").strip()
                if query_image_path and os.path.exists(query_image_path):
                    top_k = input("è¿”å›ç»“æœæ•°é‡ (é»˜è®¤: 9): ").strip()
                    top_k = int(top_k) if top_k.isdigit() else 9
                    
                    print(f"\nğŸ” å¼€å§‹ä»¥å›¾æœå›¾: '{os.path.basename(query_image_path)}'")
                    results = retrieval_system.search_by_image(query_image_path, top_k)
                    
                    if results:
                        for result in results:
                            result['query_type'] = 'image'
                        
                        display_search_results(results, f"æŸ¥è¯¢å›¾ç‰‡: {os.path.basename(query_image_path)}")
                        
                        show_viz = input("ğŸ“Š æ˜¾ç¤ºå›¾ç‰‡ç»“æœ? (y/n): ").lower() == 'y'
                        if show_viz:
                            viz_tool.show_search_results(f"æŸ¥è¯¢å›¾ç‰‡: {os.path.basename(query_image_path)}", results, max_display=top_k)
                    else:
                        print("âŒ æœªæ‰¾åˆ°ç›¸ä¼¼å›¾ç‰‡")
                else:
                    print("âŒ å›¾ç‰‡æ–‡ä»¶ä¸å­˜åœ¨")
            
            # æ–°å¢é€‰é¡¹5ï¼šæ‰‹åŠ¨æ£€æŸ¥æ›´æ–°
            elif choice == '5':
                print("\nğŸ” æ‰‹åŠ¨æ£€æŸ¥æ•°æ®æ›´æ–°...")
                try:
                    status = retrieval_system.check_index_status()
                    update_info = status['update_info']
                    
                    print(f"\nğŸ“Š æ£€æŸ¥ç»“æœ:")
                    print(f"   {update_info['message']}")
                    print(f"   ChromaDBç´¢å¼•: {status['indexed_count']:,} æ¡")
                    print(f"   æ•°æ®åº“è®°å½•: {status['database_count']:,} æ¡")
                    
                    if status['need_rebuild']:
                        print(f"\nâš ï¸ å»ºè®®é‡å»ºåŸå› :")
                        for reason in status['rebuild_reason']:
                            print(f"   â€¢ {reason}")
                        
                        rebuild = input("\nğŸ¤” æ˜¯å¦ç«‹å³é‡å»ºç´¢å¼•? (y/n): ").lower() == 'y'
                        if rebuild:
                            retrieval_system.smart_index_management(force_rebuild=True)
                    else:
                        print("âœ… ç´¢å¼•çŠ¶æ€è‰¯å¥½ï¼Œæ— éœ€é‡å»º")
                        
                except Exception as e:
                    print(f"âŒ æ£€æŸ¥å¤±è´¥: {e}")
            
            # åŸæœ‰çš„é€‰é¡¹6æ”¹ä¸ºæ£€æŸ¥æ•°æ®å®Œæ•´æ€§
            elif choice == '6':
                print("\nğŸ” æ£€æŸ¥æ•°æ®å®Œæ•´æ€§...")
                try:
                    integrity_report = retrieval_system.check_data_integrity()
                    if integrity_report['missing_count'] > 0:
                        print(f"\nå‘ç° {integrity_report['missing_count']} æ¡ç¼ºå¤±æ•°æ®")
                        print("å»ºè®®é‡å»ºç´¢å¼•ä»¥ç¡®ä¿æ•°æ®å®Œæ•´æ€§")
                    else:
                        print("âœ… æ•°æ®å®Œæ•´æ€§è‰¯å¥½!")
                except Exception as e:
                    print(f"âŒ æ£€æŸ¥å¤±è´¥: {e}")
            
            # æ–°å¢é€‰é¡¹7ï¼šå¼ºåˆ¶é‡å»ºç´¢å¼•
            elif choice == '7':
                print("\nğŸ”„ å¼ºåˆ¶é‡å»ºç´¢å¼•...")
                try:
                    confirm = input("âš ï¸ è¿™å°†åˆ é™¤ç°æœ‰ç´¢å¼•å¹¶é‡æ–°æ„å»ºï¼Œç¡®è®¤å—? (y/n): ").lower()
                    if confirm == 'y':
                        retrieval_system.smart_index_management(force_rebuild=True)
                    else:
                        print("âŒ å·²å–æ¶ˆ")
                except Exception as e:
                    print(f"âŒ é‡å»ºå¤±è´¥: {e}")
                
                
            elif choice == '8':
                # ç³»ç»ŸçŠ¶æ€
                system_info = retrieval_system.get_system_info()
                print("\nğŸ“Š è¯¦ç»†ç³»ç»ŸçŠ¶æ€:")
                print(f"   LLMé›†æˆ: {'âœ…' if retrieval_system.openrouter else 'âŒ'}")
                if retrieval_system.openrouter:
                    print(f"   LLMæ¨¡å‹: {openrouter_model}")
                print(f"   CLIPæ¨¡å‹: {system_info['clip_model']}")
                print(f"   ç‰¹å¾ç»´åº¦: {system_info['feature_dim']}")
                print(f"   è¿è¡Œè®¾å¤‡: {system_info['device']}")
                print(f"   å·²ç´¢å¼•: {system_info['collection']['count']} å¼ å›¾ç‰‡")
                print(f"   å­˜å‚¨æ¨¡å¼: {system_info['collection'].get('mode', 'unknown')}")
                print(f"   å¯ç”¨æ ‡ç­¾: {len(retrieval_system.available_tags)} ä¸ª")
                
                # æ˜¾ç¤ºæ•°æ®åº“å­—æ®µä¿¡æ¯
                if 'dataset' in system_info:
                    dataset_info = system_info['dataset']
                    print(f"   æ•°æ®åº“å­—æ®µ: {dataset_info.get('available_tag_fields', [])}")
                    print(f"   æ ‡ç­¾ç»Ÿè®¡: {dataset_info.get('tag_stats', {})}")
                
            elif choice == '9':
                print("\nğŸ‘‹ æ„Ÿè°¢ä½¿ç”¨æ™ºèƒ½æ£€ç´¢ç³»ç»Ÿ!")
                break
            
            else:
                print("âŒ æ— æ•ˆé€‰æ‹©")
                
    
    except Exception as e:
        print(f"\nâŒ ç³»ç»Ÿé”™è¯¯: {e}")
        logger.exception("ç³»ç»Ÿé”™è¯¯")
    
    finally:
        try:
            retrieval_system.close_connections()
        except:
            pass

if __name__ == "__main__":
    main()